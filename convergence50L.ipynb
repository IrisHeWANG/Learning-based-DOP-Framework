{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiezhq/.wanghe_env/lib/python3.7/site-packages/torch_sparse/tensor.py:46: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  index = mat.nonzero()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import copy\n",
    "import pandas as pd\n",
    "import xlwt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_nodes = 5\n",
    "num_edges = 6\n",
    "n = 100\n",
    "m = 300\n",
    "k = 60\n",
    "train_num = 1000\n",
    "test_num = 100\n",
    "num_layers = 50\n",
    "nnz = 30\n",
    "\n",
    "#less nnz =5; m = 50; k = 10\n",
    "\n",
    "def metropolis(adjacency_matrix):\n",
    "    num_of_nodes = adjacency_matrix.shape[0]\n",
    "    metropolis=np.zeros((num_of_nodes,num_of_nodes))\n",
    "    for i in range(num_of_nodes):\n",
    "        for j in range(num_of_nodes):\n",
    "            if adjacency_matrix[i,j]==1:\n",
    "                d_i = np.sum(adjacency_matrix[i,:])\n",
    "                d_j = np.sum(adjacency_matrix[j,:])\n",
    "                metropolis[i,j]=1/(1+max(d_i,d_j))\n",
    "        metropolis[i,i]=1-sum(metropolis[i,:])\n",
    "    return metropolis\n",
    "\n",
    "class SynDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples\n",
    "        self.A = []; \n",
    "        self.y = []; \n",
    "        self.x_true = []\n",
    "        self.pyg_data=[]\n",
    "        self.process()\n",
    "        \n",
    "        \n",
    "    def gen_func(self, num_of_nodes, n, m, k):\n",
    "        A_all = np.random.randn(m, n)\n",
    "        x = np.random.randn(n)\n",
    "        x_norm = 0\n",
    "\n",
    "        while(x_norm < 1e-2):\n",
    "            x_mask = np.random.rand(n)\n",
    "            x_mask[x_mask < 1 - nnz/100] = 0\n",
    "            x_mask[x_mask > 0] = 1\n",
    "            x_norm = np.linalg.norm(x * x_mask)\n",
    "\n",
    "        x = x * x_mask\n",
    "        x = x/np.linalg.norm(x)\n",
    "        \n",
    "        SNR_db = 50\n",
    "        SNR = 10**(SNR_db/10)\n",
    "        \n",
    "        noise = np.random.randn(m) * np.sqrt(1/SNR)\n",
    "        y_all = A_all@x + noise\n",
    "\n",
    "        A = np.zeros((num_of_nodes, k , n))\n",
    "        y = np.zeros((num_of_nodes, k))\n",
    "        for ii in range(num_of_nodes):\n",
    "            start = (k*ii) % m; end = (k*(ii+1) )%m\n",
    "            if(start > end):\n",
    "                A[ii,:,:] = np.concatenate((A_all[start:,:],A_all[:end,:]), axis = 0)\n",
    "                y[ii,:] = np.concatenate((np.expand_dims(y_all[start:], axis = 0), \n",
    "                                          np.expand_dims(y_all[:end], axis = 0)), axis = 1)\n",
    "            else:\n",
    "                A[ii,:,:] = A_all[start:end,:]\n",
    "                y[ii,:] = np.expand_dims(y_all[start:end], axis = 0)\n",
    "                \n",
    "        x = np.expand_dims(x, axis = 0)\n",
    "        x = x.repeat(num_of_nodes, axis = 0)\n",
    "        \n",
    "        return A, y, x\n",
    "\n",
    "    def gen_graph(self, num_of_nodes, num_of_edges, directed=False, add_self_loops=True):\n",
    "        G = nx.gnm_random_graph(num_of_nodes, num_of_edges, directed=directed)\n",
    "        k = 0\n",
    "        while (nx.is_strongly_connected(G) if directed else nx.is_connected(G)) == False:\n",
    "            G = nx.gnm_random_graph(num_of_nodes, num_of_edges, directed=directed)\n",
    "            k += 1\n",
    "        # print(\"Check if connected: \", nx.is_connected(G))\n",
    "        # nx.draw(G)\n",
    "        \n",
    "        edge_index = from_networkx(G).edge_index\n",
    "        adj = nx.to_numpy_matrix(G)\n",
    "        return G, adj,edge_index\n",
    "        \n",
    "    def process(self):\n",
    "        _, adj,edge_index = self.gen_graph(num_nodes, num_edges)\n",
    "        self.edge_index = edge_index\n",
    "        W = metropolis(adj)\n",
    "        self.W = [torch.tensor(W, dtype = torch.float)] * self.samples\n",
    "        \n",
    "        \n",
    "        for ii in range(self.samples):\n",
    "            A, y, x_true = self.gen_func(num_nodes, n, m, k)\n",
    "            self.A.append(torch.tensor(A, dtype = torch.float) ); \n",
    "            self.y.append(torch.tensor(y, dtype = torch.float) ); \n",
    "            self.x_true.append(torch.tensor(x_true, dtype = torch.float) )\n",
    "            \n",
    "            edge_weight=torch.tensor(W,dtype=torch.float)\n",
    "            self.pyg_data.append(Data(edge_weight=SparseTensor.from_dense(edge_weight)))        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.W[idx], self.A[idx], self.y[idx], self.x_true[idx], self.pyg_data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of graphs in the dataset\"\"\"\n",
    "        return len(self.A)\n",
    "    \n",
    "    \n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    W, A, y, x_true, pyg_data = map(list, zip(*samples))\n",
    "    W = torch.stack(W)\n",
    "    A = torch.stack(A)\n",
    "    y = torch.stack(y)\n",
    "    x_true = torch.stack(x_true)\n",
    "    pyg_data = Batch.from_data_list(pyg_data)\n",
    "    return W, A, y, x_true, pyg_data\n",
    "class MetropolisConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super(MetropolisConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "\n",
    "    def forward(self, x, pyg_data):\n",
    "        (B, N, D)=x.shape\n",
    "        out = self.propagate(x=x.view(-1,D), edge_index=pyg_data.edge_weight, node_dim=-1)\n",
    "        return out.view(B,N,D)\n",
    "\n",
    "    def message_and_aggregate(self, adj_t, x):\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "def step_loss(gamma,x, y):\n",
    "    #gamma = 0.75\n",
    "    n_steps = x.shape[0]\n",
    "    #print(n_steps)\n",
    "    di = torch.ones((n_steps)) * gamma\n",
    "    power = torch.tensor(range(n_steps, 0, -1))\n",
    "    gamma_a = di ** power\n",
    "    gamma_a = gamma_a.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    y = torch.unsqueeze(y, axis = 0)\n",
    "    ele_loss = gamma_a * (x - y) **2\n",
    "    #print(ele_loss.shape)\n",
    "    #print(torch.mean(ele_loss,  (1,2,3) ))\n",
    "    loss = torch.mean(ele_loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "train_data = SynDataset(train_num)\n",
    "val_data = SynDataset(test_num)\n",
    "test_data = SynDataset(test_num)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_data, batch_size=100, shuffle=False, collate_fn=collate)\n",
    "test_loader = DataLoader(test_data, batch_size=100, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN-PGEXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00013569310976890847 tensor(0.0082, grad_fn=<SelectBackward>) tensor(0.0007, grad_fn=<SelectBackward>)\n",
      "1.0673586157139425e-06 tensor(0.0051, grad_fn=<SelectBackward>) tensor(0.0026, grad_fn=<SelectBackward>)\n",
      "4.2311402026484757e-07 tensor(0.0045, grad_fn=<SelectBackward>) tensor(0.0029, grad_fn=<SelectBackward>)\n",
      "2.95198097521876e-07 tensor(0.0040, grad_fn=<SelectBackward>) tensor(0.0032, grad_fn=<SelectBackward>)\n",
      "2.3132930948577268e-07 tensor(0.0036, grad_fn=<SelectBackward>) tensor(0.0034, grad_fn=<SelectBackward>)\n",
      "1.8706502791232538e-07 tensor(0.0032, grad_fn=<SelectBackward>) tensor(0.0035, grad_fn=<SelectBackward>)\n",
      "1.5605867087487013e-07 tensor(0.0029, grad_fn=<SelectBackward>) tensor(0.0037, grad_fn=<SelectBackward>)\n",
      "1.340106055014445e-07 tensor(0.0027, grad_fn=<SelectBackward>) tensor(0.0038, grad_fn=<SelectBackward>)\n",
      "1.1711095559974183e-07 tensor(0.0024, grad_fn=<SelectBackward>) tensor(0.0040, grad_fn=<SelectBackward>)\n",
      "1.060343715053591e-07 tensor(0.0023, grad_fn=<SelectBackward>) tensor(0.0041, grad_fn=<SelectBackward>)\n",
      "1.1217327799961652e-07 tensor(0.0034, grad_fn=<SelectBackward>) tensor(0.0040, grad_fn=<SelectBackward>)\n",
      "1.0590313692659947e-07 tensor(0.0037, grad_fn=<SelectBackward>) tensor(0.0041, grad_fn=<SelectBackward>)\n",
      "1.1929938703580945e-07 tensor(0.0046, grad_fn=<SelectBackward>) tensor(0.0041, grad_fn=<SelectBackward>)\n",
      "9.874083750638363e-08 tensor(0.0045, grad_fn=<SelectBackward>) tensor(0.0042, grad_fn=<SelectBackward>)\n",
      "1.0848608100744173e-07 tensor(0.0057, grad_fn=<SelectBackward>) tensor(0.0041, grad_fn=<SelectBackward>)\n",
      "1.0304272435313067e-07 tensor(0.0061, grad_fn=<SelectBackward>) tensor(0.0042, grad_fn=<SelectBackward>)\n",
      "1.0028954200436146e-07 tensor(0.0066, grad_fn=<SelectBackward>) tensor(0.0042, grad_fn=<SelectBackward>)\n",
      "9.43396112340622e-08 tensor(0.0065, grad_fn=<SelectBackward>) tensor(0.0043, grad_fn=<SelectBackward>)\n",
      "2.3531141324362181e-07 tensor(0.0085, grad_fn=<SelectBackward>) tensor(0.0039, grad_fn=<SelectBackward>)\n",
      "9.269548972845598e-08 tensor(0.0074, grad_fn=<SelectBackward>) tensor(0.0044, grad_fn=<SelectBackward>)\n",
      "1.2181697917057477e-07 tensor(0.0081, grad_fn=<SelectBackward>) tensor(0.0043, grad_fn=<SelectBackward>)\n",
      "8.325274514220382e-08 tensor(0.0072, grad_fn=<SelectBackward>) tensor(0.0046, grad_fn=<SelectBackward>)\n",
      "1.6335093233621478e-07 tensor(0.0095, grad_fn=<SelectBackward>) tensor(0.0038, grad_fn=<SelectBackward>)\n",
      "1.0907373870772119e-07 tensor(0.0091, grad_fn=<SelectBackward>) tensor(0.0043, grad_fn=<SelectBackward>)\n",
      "8.66791121101329e-08 tensor(0.0087, grad_fn=<SelectBackward>) tensor(0.0046, grad_fn=<SelectBackward>)\n",
      "2.4522418096140086e-07 tensor(0.0112, grad_fn=<SelectBackward>) tensor(0.0035, grad_fn=<SelectBackward>)\n",
      "1.1871409366648322e-07 tensor(0.0110, grad_fn=<SelectBackward>) tensor(0.0042, grad_fn=<SelectBackward>)\n",
      "9.225522479283654e-08 tensor(0.0105, grad_fn=<SelectBackward>) tensor(0.0046, grad_fn=<SelectBackward>)\n",
      "1.1317539105881735e-07 tensor(0.0112, grad_fn=<SelectBackward>) tensor(0.0043, grad_fn=<SelectBackward>)\n",
      "8.304715182205769e-08 tensor(0.0106, grad_fn=<SelectBackward>) tensor(0.0048, grad_fn=<SelectBackward>)\n",
      "9.202755180126587e-08 tensor(0.0110, grad_fn=<SelectBackward>) tensor(0.0046, grad_fn=<SelectBackward>)\n",
      "7.932599821458552e-08 tensor(0.0106, grad_fn=<SelectBackward>) tensor(0.0048, grad_fn=<SelectBackward>)\n",
      "8.904916071195146e-08 tensor(0.0113, grad_fn=<SelectBackward>) tensor(0.0047, grad_fn=<SelectBackward>)\n",
      "7.511349786604171e-08 tensor(0.0108, grad_fn=<SelectBackward>) tensor(0.0049, grad_fn=<SelectBackward>)\n",
      "1.0152031437726805e-07 tensor(0.0120, grad_fn=<SelectBackward>) tensor(0.0045, grad_fn=<SelectBackward>)\n",
      "8.165739728838162e-08 tensor(0.0110, grad_fn=<SelectBackward>) tensor(0.0049, grad_fn=<SelectBackward>)\n",
      "7.868931950483216e-08 tensor(0.0109, grad_fn=<SelectBackward>) tensor(0.0049, grad_fn=<SelectBackward>)\n",
      "8.211792579260191e-08 tensor(0.0110, grad_fn=<SelectBackward>) tensor(0.0049, grad_fn=<SelectBackward>)\n",
      "7.9589018042725e-08 tensor(0.0104, grad_fn=<SelectBackward>) tensor(0.0049, grad_fn=<SelectBackward>)\n",
      "8.341917290266565e-08 tensor(0.0107, grad_fn=<SelectBackward>) tensor(0.0048, grad_fn=<SelectBackward>)\n",
      "7.779531614460211e-07 tensor(0.0131, grad_fn=<SelectBackward>) tensor(0.0029, grad_fn=<SelectBackward>)\n",
      "3.891525501842352e-07 tensor(0.0130, grad_fn=<SelectBackward>) tensor(0.0032, grad_fn=<SelectBackward>)\n",
      "2.7344022557329595e-07 tensor(0.0130, grad_fn=<SelectBackward>) tensor(0.0035, grad_fn=<SelectBackward>)\n",
      "2.1665335836473787e-07 tensor(0.0129, grad_fn=<SelectBackward>) tensor(0.0036, grad_fn=<SelectBackward>)\n",
      "1.8238164134487533e-07 tensor(0.0128, grad_fn=<SelectBackward>) tensor(0.0038, grad_fn=<SelectBackward>)\n",
      "1.5889751070474745e-07 tensor(0.0127, grad_fn=<SelectBackward>) tensor(0.0040, grad_fn=<SelectBackward>)\n",
      "1.410010659341765e-07 tensor(0.0126, grad_fn=<SelectBackward>) tensor(0.0041, grad_fn=<SelectBackward>)\n",
      "1.2594992959691353e-07 tensor(0.0124, grad_fn=<SelectBackward>) tensor(0.0043, grad_fn=<SelectBackward>)\n",
      "1.1545787925726358e-07 tensor(0.0123, grad_fn=<SelectBackward>) tensor(0.0044, grad_fn=<SelectBackward>)\n",
      "1.0629801550088303e-07 tensor(0.0122, grad_fn=<SelectBackward>) tensor(0.0045, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Net_PGEXTRA(torch.nn.Module):\n",
    "    def __init__(self, step_size, num_layers):\n",
    "        super(Net_PGEXTRA, self).__init__()\n",
    "        self.step_size = nn.Parameter(torch.ones(num_layers)*step_size)\n",
    "        self.lam = nn.Parameter(torch.ones(num_layers)*step_size*10)\n",
    "        self.num_layers = num_layers\n",
    "        self.conv=MetropolisConv()\n",
    "    def tgrad_qp(self, A, b, x):\n",
    "        # A: nodes * k * n\n",
    "        # X: nodes * n\n",
    "        # Y: nodes * k\n",
    "        '''grad_A = np.zeros(x.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            grad_A[i] = A[i].T @ (A[i] @ x[i] - b[i])\n",
    "        return grad_A'''\n",
    "        x_ = torch.unsqueeze(x, axis = -1)\n",
    "        b_ = torch.unsqueeze(b, axis = -1)\n",
    "\n",
    "        A_t = A.transpose(2,3)\n",
    "        grad_A = A_t @ (A @ x_ - b_)\n",
    "        #print(A.shape, x.shape, b.shape)\n",
    "        #print(grad_A.shape)\n",
    "        grad_A = torch.squeeze(grad_A, axis = -1)\n",
    "        #print(grad_A.shape)\n",
    "        return grad_A\n",
    "    \n",
    "    def act(self, x, ii):\n",
    "        tau = self.lam[ii] #* self.step_size[ii]\n",
    "        return F.relu(x - tau) - F.relu( - x - tau)\n",
    "            \n",
    "    def forward(self, W, A, b,pyg_data, max_iter):\n",
    "        (batch_size, num_of_nodes, _, dim) = A.shape\n",
    "        init_x = torch.zeros((batch_size, num_of_nodes, dim))\n",
    "        ret_z = []\n",
    "        \n",
    "        k = 1\n",
    "        x_0 = init_x\n",
    "        x_12 = self.conv(x_0,pyg_data) - self.step_size[0] * self.tgrad_qp(A, b, x_0)\n",
    "        x_1 = self.act(x_12, 0)\n",
    "        \n",
    "        x_hist = [init_x,x_1]\n",
    "        while (k < max_iter):\n",
    "            x_32 = self.conv(x_1,pyg_data) + x_12 - (self.conv(x_0,pyg_data) + x_0)/2 - \\\n",
    "                self.step_size[k] * (self.tgrad_qp(A, b, x_1)-self.tgrad_qp(A, b, x_0))\n",
    "            x_2 = self.act(x_32, k)\n",
    "            \n",
    "            ret_z.append(x_2)\n",
    "\n",
    "            x_0 = x_1\n",
    "            x_1 = x_2\n",
    "            x_12 = x_32\n",
    "\n",
    "            k = k + 1\n",
    "            x_hist.append(x_2)\n",
    "        \n",
    "        ret_z = torch.stack(ret_z)\n",
    "        return ret_z, x_2,x_hist\n",
    "      \n",
    "###main\n",
    "model_PGEXTRA = Net_PGEXTRA(1e-3, num_layers)\n",
    "optimizer = optim.Adam(model_PGEXTRA.parameters(), lr=1e-4)\n",
    "model_PGEXTRA.train()\n",
    "epoch_losses = []\n",
    "for epoch in range(500):\n",
    "    epoch_loss = 0\n",
    "    for iter, (W, A, y, x_true,pyg_data) in enumerate(train_loader):\n",
    "        z, _,_ = model_PGEXTRA(W, A, y, pyg_data,num_layers)\n",
    "        loss = step_loss(0.81,z, x_true)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (iter + 1)\n",
    "    if(epoch % 10 == 0):\n",
    "        print(epoch_loss, model_PGEXTRA.lam[1], model_PGEXTRA.step_size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN-DGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00021758794036941254 tensor(0.0079, grad_fn=<SelectBackward>) tensor(0.0029, grad_fn=<SelectBackward>)\n",
      "2.240302615064138e-05 tensor(0.0076, grad_fn=<SelectBackward>) tensor(0.0029, grad_fn=<SelectBackward>)\n",
      "1.8076279616252577e-05 tensor(0.0076, grad_fn=<SelectBackward>) tensor(0.0029, grad_fn=<SelectBackward>)\n",
      "1.578619364295264e-05 tensor(0.0076, grad_fn=<SelectBackward>) tensor(0.0029, grad_fn=<SelectBackward>)\n",
      "1.4204976054088547e-05 tensor(0.0076, grad_fn=<SelectBackward>) tensor(0.0029, grad_fn=<SelectBackward>)\n",
      "1.2905917685657187e-05 tensor(0.0076, grad_fn=<SelectBackward>) tensor(0.0029, grad_fn=<SelectBackward>)\n",
      "1.1948518391591278e-05 tensor(0.0076, grad_fn=<SelectBackward>) tensor(0.0029, grad_fn=<SelectBackward>)\n",
      "1.102023611565528e-05 tensor(0.0076, grad_fn=<SelectBackward>) tensor(0.0029, grad_fn=<SelectBackward>)\n",
      "1.0261077164841481e-05 tensor(0.0076, grad_fn=<SelectBackward>) tensor(0.0029, grad_fn=<SelectBackward>)\n",
      "9.538237748074607e-06 tensor(0.0075, grad_fn=<SelectBackward>) tensor(0.0030, grad_fn=<SelectBackward>)\n",
      "8.874980593986947e-06 tensor(0.0075, grad_fn=<SelectBackward>) tensor(0.0030, grad_fn=<SelectBackward>)\n",
      "8.314537552678303e-06 tensor(0.0075, grad_fn=<SelectBackward>) tensor(0.0030, grad_fn=<SelectBackward>)\n",
      "7.804651872334034e-06 tensor(0.0075, grad_fn=<SelectBackward>) tensor(0.0030, grad_fn=<SelectBackward>)\n",
      "7.310984557307165e-06 tensor(0.0075, grad_fn=<SelectBackward>) tensor(0.0031, grad_fn=<SelectBackward>)\n",
      "6.911499113471109e-06 tensor(0.0075, grad_fn=<SelectBackward>) tensor(0.0031, grad_fn=<SelectBackward>)\n",
      "6.423866182103666e-06 tensor(0.0075, grad_fn=<SelectBackward>) tensor(0.0031, grad_fn=<SelectBackward>)\n",
      "6.026670632763853e-06 tensor(0.0074, grad_fn=<SelectBackward>) tensor(0.0032, grad_fn=<SelectBackward>)\n",
      "5.848887738579833e-06 tensor(0.0074, grad_fn=<SelectBackward>) tensor(0.0032, grad_fn=<SelectBackward>)\n",
      "5.355395131800833e-06 tensor(0.0074, grad_fn=<SelectBackward>) tensor(0.0033, grad_fn=<SelectBackward>)\n",
      "5.686014475259071e-06 tensor(0.0073, grad_fn=<SelectBackward>) tensor(0.0033, grad_fn=<SelectBackward>)\n",
      "4.121997605466277e-06 tensor(0.0073, grad_fn=<SelectBackward>) tensor(0.0034, grad_fn=<SelectBackward>)\n",
      "3.946714201674695e-06 tensor(0.0073, grad_fn=<SelectBackward>) tensor(0.0034, grad_fn=<SelectBackward>)\n",
      "3.784322586852795e-06 tensor(0.0072, grad_fn=<SelectBackward>) tensor(0.0035, grad_fn=<SelectBackward>)\n",
      "3.6125931757169383e-06 tensor(0.0072, grad_fn=<SelectBackward>) tensor(0.0036, grad_fn=<SelectBackward>)\n",
      "3.5275268928103287e-06 tensor(0.0071, grad_fn=<SelectBackward>) tensor(0.0037, grad_fn=<SelectBackward>)\n",
      "3.408440427676851e-06 tensor(0.0071, grad_fn=<SelectBackward>) tensor(0.0038, grad_fn=<SelectBackward>)\n",
      "3.2827203071406075e-06 tensor(0.0071, grad_fn=<SelectBackward>) tensor(0.0039, grad_fn=<SelectBackward>)\n",
      "3.210838372069702e-06 tensor(0.0070, grad_fn=<SelectBackward>) tensor(0.0040, grad_fn=<SelectBackward>)\n",
      "3.178013557203485e-06 tensor(0.0070, grad_fn=<SelectBackward>) tensor(0.0041, grad_fn=<SelectBackward>)\n",
      "3.1164435583264094e-06 tensor(0.0069, grad_fn=<SelectBackward>) tensor(0.0043, grad_fn=<SelectBackward>)\n",
      "3.1581502355493285e-06 tensor(0.0068, grad_fn=<SelectBackward>) tensor(0.0045, grad_fn=<SelectBackward>)\n",
      "2.9940633936575978e-06 tensor(0.0068, grad_fn=<SelectBackward>) tensor(0.0047, grad_fn=<SelectBackward>)\n",
      "2.6514257598364566e-06 tensor(0.0068, grad_fn=<SelectBackward>) tensor(0.0050, grad_fn=<SelectBackward>)\n",
      "2.4249003018894655e-06 tensor(0.0068, grad_fn=<SelectBackward>) tensor(0.0053, grad_fn=<SelectBackward>)\n",
      "2.3208290897969164e-06 tensor(0.0068, grad_fn=<SelectBackward>) tensor(0.0056, grad_fn=<SelectBackward>)\n",
      "2.2447686092164076e-06 tensor(0.0069, grad_fn=<SelectBackward>) tensor(0.0060, grad_fn=<SelectBackward>)\n",
      "2.1402125618408263e-06 tensor(0.0068, grad_fn=<SelectBackward>) tensor(0.0064, grad_fn=<SelectBackward>)\n",
      "1.0770074453603229e-05 tensor(0.0102, grad_fn=<SelectBackward>) tensor(0.0044, grad_fn=<SelectBackward>)\n",
      "5.917972714541975e-06 tensor(0.0100, grad_fn=<SelectBackward>) tensor(0.0051, grad_fn=<SelectBackward>)\n",
      "4.424496722776894e-06 tensor(0.0098, grad_fn=<SelectBackward>) tensor(0.0056, grad_fn=<SelectBackward>)\n",
      "3.5720470776823277e-06 tensor(0.0097, grad_fn=<SelectBackward>) tensor(0.0061, grad_fn=<SelectBackward>)\n",
      "3.003410682822505e-06 tensor(0.0096, grad_fn=<SelectBackward>) tensor(0.0065, grad_fn=<SelectBackward>)\n",
      "2.5705553170496387e-06 tensor(0.0094, grad_fn=<SelectBackward>) tensor(0.0070, grad_fn=<SelectBackward>)\n",
      "2.253966165710608e-06 tensor(0.0093, grad_fn=<SelectBackward>) tensor(0.0074, grad_fn=<SelectBackward>)\n",
      "1.9902491175116666e-06 tensor(0.0092, grad_fn=<SelectBackward>) tensor(0.0079, grad_fn=<SelectBackward>)\n",
      "1.7467478663490965e-06 tensor(0.0090, grad_fn=<SelectBackward>) tensor(0.0084, grad_fn=<SelectBackward>)\n",
      "1.5800587682690548e-06 tensor(0.0089, grad_fn=<SelectBackward>) tensor(0.0090, grad_fn=<SelectBackward>)\n",
      "1.435345627243123e-06 tensor(0.0088, grad_fn=<SelectBackward>) tensor(0.0096, grad_fn=<SelectBackward>)\n",
      "1.306352896079943e-06 tensor(0.0087, grad_fn=<SelectBackward>) tensor(0.0102, grad_fn=<SelectBackward>)\n",
      "1.1981159246943207e-06 tensor(0.0086, grad_fn=<SelectBackward>) tensor(0.0108, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Net_DGD(torch.nn.Module):\n",
    "    def __init__(self, step_size, num_layers):\n",
    "        super(Net_DGD, self).__init__()\n",
    "        self.step_size = nn.Parameter(torch.ones(num_layers)*step_size)\n",
    "        self.lam = nn.Parameter(torch.ones(num_layers)*step_size*10)\n",
    "        self.num_layers = num_layers\n",
    "        self.conv=MetropolisConv()\n",
    "    def tgrad_qp(self, A, b, x):\n",
    "        # A: nodes * k * n\n",
    "        # X: nodes * n\n",
    "        # Y: nodes * k\n",
    "        '''grad_A = np.zeros(x.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            grad_A[i] = A[i].T @ (A[i] @ x[i] - b[i])\n",
    "        return grad_A'''\n",
    "        x_ = torch.unsqueeze(x, axis = -1)\n",
    "        b_ = torch.unsqueeze(b, axis = -1)\n",
    "\n",
    "        A_t = A.transpose(2,3)\n",
    "        grad_A = A_t @ (A @ x_ - b_)\n",
    "        #print(A.shape, x.shape, b.shape)\n",
    "        #print(grad_A.shape)\n",
    "        grad_A = torch.squeeze(grad_A, axis = -1)\n",
    "        #print(grad_A.shape)\n",
    "        return grad_A\n",
    "    \n",
    "    def act(self, x, ii):\n",
    "        tau = self.lam[ii] #* self.step_size[ii]\n",
    "        return F.relu(x - tau) - F.relu( - x - tau)\n",
    "            \n",
    "    def forward(self, W, A, b,pyg_data, max_iter):\n",
    "        (batch_size, num_of_nodes, _, dim) = A.shape\n",
    "        init_x = torch.zeros((batch_size, num_of_nodes, dim))\n",
    "        ret_z = []\n",
    "        \n",
    "        k = 1\n",
    "        x_0 = init_x\n",
    "        x_12 = self.conv(x_0,pyg_data) - self.step_size[0] * self.tgrad_qp(A, b, x_0)\n",
    "        x_1 = self.act(x_12, 0)\n",
    "        \n",
    "        x_hist = [init_x,x_1]\n",
    "        while (k < max_iter):\n",
    "            #x_32 = self.conv(x_1,pyg_data) + x_12 - (self.conv(x_0,pyg_data) + x_0)/2 - \\\n",
    "            #    self.step_size[k] * (self.tgrad_qp(A, b, x_1)-self.tgrad_qp(A, b, x_0))\n",
    "            x_32 = self.conv(x_1,pyg_data) - self.step_size[k] * self.tgrad_qp(A, b, x_1)\n",
    "            x_2 = self.act(x_32, k)\n",
    "            \n",
    "            ret_z.append(x_2)\n",
    "\n",
    "            x_0 = x_1\n",
    "            x_1 = x_2\n",
    "            x_12 = x_32\n",
    "\n",
    "            k = k + 1\n",
    "            x_hist.append(x_2)\n",
    "        \n",
    "        ret_z = torch.stack(ret_z)\n",
    "        return ret_z, x_2,x_hist\n",
    "\n",
    "\n",
    "model_DGD = Net_DGD(1e-3, num_layers)\n",
    "optimizer = optim.Adam(model_DGD.parameters(), lr=1e-4)\n",
    "model_DGD.train()\n",
    "epoch_losses = []\n",
    "for epoch in range(500):\n",
    "    epoch_loss = 0\n",
    "    for iter, (W, A, y, x_true,pyg_data) in enumerate(train_loader):\n",
    "        z, _,_ = model_DGD(W, A, y, pyg_data,num_layers)\n",
    "        loss = step_loss(0.85,z, x_true)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (iter + 1)\n",
    "    if(epoch % 10 == 0):\n",
    "        print(epoch_loss, model_DGD.lam[1], model_DGD.step_size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Origin Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tgrad_qp(A, b, x):\n",
    "    # A: nodes * k * n\n",
    "    # X: nodes * n\n",
    "    # Y: nodes * k\n",
    "    '''grad_A = np.zeros(x.shape)\n",
    "    for i in range(x.shape[0]):\n",
    "        grad_A[i] = A[i].T @ (A[i] @ x[i] - b[i])\n",
    "    return grad_A'''\n",
    "    x_ = torch.unsqueeze(x, axis = -1)\n",
    "    b_ = torch.unsqueeze(b, axis = -1)\n",
    "    \n",
    "    A_t = A.transpose(2,3)\n",
    "    grad_A = A_t @ (A @ x_ - b_)\n",
    "    # print(A.shape, x.shape, b.shape)\n",
    "    grad_A = torch.squeeze(grad_A, axis = -1)\n",
    "    return grad_A\n",
    "\n",
    "def torch_soft(x, tau):\n",
    "    return F.relu(x - tau) - F.relu( - x - tau)\n",
    "\n",
    "def opt_distance(x,opt):\n",
    "    error = 0\n",
    "    batch_size = x.shape[0]\n",
    "    num_of_nodes = x.shape[1]\n",
    "    error = np.linalg.norm(x-opt)**2\n",
    "    return error/num_of_nodes/batch_size\n",
    "\n",
    "def hist_nmse(x_hist,opt):\n",
    "    error = []\n",
    "    iteration = len(x_hist)\n",
    "    #print(iteration)\n",
    "    for k in range(iteration):\n",
    "        error.append(10*np.log10(opt_distance(x_hist[k].detach(),opt)))\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origin PG-EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 0.01 \t 0.12945430286414922 \t 0.03877498257876323\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 0.05 \t 0.12919341212205654 \t 0.03792135231036855\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 0.1 \t 0.12892956855315607 \t 0.036975312114520424\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 0.5 \t 0.12932454546207917 \t 0.03428692882305041\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 1 \t 0.13535670942114847 \t 0.03977651463635266\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 5 \t 0.28984356282424595 \t 0.2055009911531106\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 0.01 \t 0.07603007731333537 \t 0.01747969873558759\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 0.05 \t 0.0754209575523605 \t 0.016576888784421043\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 0.1 \t 0.07474511489435827 \t 0.015627076364378923\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 0.5 \t 0.07282081903735571 \t 0.01413440738264501\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 1 \t 0.07776996984987636 \t 0.020874500112481202\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 5 \t 0.2379083047128006 \t 0.18987220684992098\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 0.01 \t 0.03819573177033635 \t 0.00600200690444197\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 0.05 \t 0.03734266887396416 \t 0.005260784880437115\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 0.1 \t 0.036400388951013156 \t 0.004593354453934381\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 0.5 \t 0.03370471245225462 \t 0.005417860678436342\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 1 \t 0.039106176317045535 \t 0.012957194063392422\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 5 \t 0.20462755972516788 \t 0.18358430004518594\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 0.01 \t 0.0057010769521594964 \t 0.00023510867493307507\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 0.05 \t 0.004966532868255058 \t 0.0001221531825622617\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 0.1 \t 0.004312551871828674 \t 0.0001696870020133474\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 0.5 \t 0.0051788497819798066 \t 0.0027576684842756547\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 1 \t 0.012682275035356839 \t 0.010422157550945485\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 5 \t 0.18336186363271556 \t 0.18174261362729158\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 0.01 \t 296609074.4590332 \t 2.671181670624344e+22\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 0.05 \t 268811879.1257988 \t 2.411480024764376e+22\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 0.1 \t 236052303.18211132 \t 2.1053545944708307e+22\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 0.5 \t 61035495.946113765 \t 4.950616375254757e+21\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 1 \t 2256195.179522583 \t 8.861256753373389e+18\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 5 \t 0.18172239572107354 \t 0.18172126849681264\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 0.01 \t inf \t nan\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 0.05 \t inf \t nan\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 0.1 \t inf \t nan\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 0.5 \t inf \t nan\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 1 \t inf \t nan\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 5 \t inf \t nan\n"
     ]
    }
   ],
   "source": [
    "def torch_PGEXTRA(W, A, b, max_iter, step_size,tau):\n",
    "    (batch_size, num_of_nodes, _, dim) = A.shape\n",
    "    init_x = torch.zeros((batch_size, num_of_nodes, dim))\n",
    "    \n",
    "    \n",
    "    (batch_size, num_of_nodes, dim) = init_x.shape\n",
    "    I = torch.unsqueeze(torch.eye(num_of_nodes), axis = 0)\n",
    "    I = I.repeat(batch_size, 1, 1)\n",
    "    \n",
    "    W_hat = (W + I)/2\n",
    "    \n",
    "    #initialization\n",
    "    k = 1\n",
    "    x_0 = init_x\n",
    "    x_12 = W @ x_0 - step_size * tgrad_qp(A, b, x_0)\n",
    "    x_1 = torch_soft(x_12, tau*step_size)\n",
    "    \n",
    "    x_hist = [init_x,x_1] #add for plot\n",
    "    while (k < max_iter):\n",
    "        \n",
    "        x_32 = W@x_1 + x_12 - W_hat@x_0 - \\\n",
    "            step_size*(tgrad_qp(A, b, x_1)-tgrad_qp(A, b, x_0))\n",
    "        x_2 = torch_soft(x_32, tau*step_size)\n",
    "        \n",
    "        x_0 = x_1\n",
    "        x_1 = x_2\n",
    "        x_12 = x_32\n",
    "        \n",
    "        k = k + 1\n",
    "        \n",
    "        x_hist.append(x_2)\n",
    "        \n",
    "    return x_2,x_hist\n",
    "\n",
    "lams = [5e-4,7e-4,1e-3, 2e-3,5e-3,1e-2]\n",
    "taus = [1e-2, 5e-2,1e-1,5e-1, 1, 5]\n",
    "best_error = 100\n",
    "best_par = {}\n",
    "for lam in lams:\n",
    "    for tau in taus:\n",
    "        for iter, (W, A, y, x_true,pyg_data) in enumerate(val_loader):\n",
    "            original,origin_hist = torch_PGEXTRA(W, A, y, 100, lam, tau)\n",
    "            loss2 = opt_distance(original.detach().numpy(), x_true.numpy())\n",
    "            loss1 = opt_distance(origin_hist[num_layers].detach().numpy(),x_true.numpy())\n",
    "            \n",
    "            print(\"lamb\\ttau\\tlayer_loss\\t\\tfinal_loss\")\n",
    "            print(lam,'\\t', tau, '\\t',loss1,'\\t',loss2)\n",
    "            \n",
    "            if loss2 < best_error:\n",
    "                best_par['lam'] = lam\n",
    "                best_par['tau'] = tau\n",
    "                best_error = loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lam': 0.002, 'tau': 0.05}\n"
     ]
    }
   ],
   "source": [
    "print(best_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origin DGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 0.01 \t 0.14081849129684268 \t 0.04393912575563445\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 0.05 \t 0.14063465843472112 \t 0.04308331741417533\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 0.1 \t 0.14046724949500639 \t 0.042143241898804265\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 0.5 \t 0.14153454898979542 \t 0.03951941047679111\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 1 \t 0.14816127124690864 \t 0.04519852388196159\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0005 \t 5 \t 0.3028532438042912 \t 0.21336901522718837\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 0.01 \t 0.0873854390768106 \t 0.02160874662677088\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 0.05 \t 0.08683453568031109 \t 0.020643760508042872\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 0.1 \t 0.08623619616968835 \t 0.01962735824114145\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 0.5 \t 0.08487679048253813 \t 0.017849331852553517\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 1 \t 0.09038458539462273 \t 0.024871025446465068\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.0007 \t 5 \t 0.2521430211850675 \t 0.19841266223439744\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 0.01 \t 0.048473075912298096 \t 0.008747484318976716\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 0.05 \t 0.04761430168943479 \t 0.007858042963670441\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 0.1 \t 0.04667457598508236 \t 0.00702662037549888\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 0.5 \t 0.04408737857408096 \t 0.007438668178439484\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 1 \t 0.04990827955909663 \t 0.015640941483323446\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.001 \t 5 \t 0.21960093778259943 \t 0.19375091610479284\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 0.01 \t 0.01145909606057387 \t 0.0008133959189533542\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 0.05 \t 0.010437648499187276 \t 0.00048301084312158337\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 0.1 \t 0.009463048038069702 \t 0.0004276056721507064\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 0.5 \t 0.009482813228032795 \t 0.0035091731711510196\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 1 \t 0.018201654362837983 \t 0.012714877192593576\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.002 \t 5 \t 0.20199511665989303 \t 0.19900221651194624\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 0.01 \t 182054662156339.2 \t 7.73563955543455e+33\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 0.05 \t 170495559175012.38 \t 7.156468107913534e+33\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 0.1 \t 156583601114316.8 \t 6.460200480061337e+33\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 0.5 \t 72228189631039.48 \t 2.417844385679781e+33\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 1 \t 21094823194300.926 \t 3.6234833154091314e+32\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.005 \t 5 \t 0.21715069648031385 \t 0.21713340602996323\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 0.01 \t inf \t nan\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 0.05 \t inf \t nan\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 0.1 \t inf \t nan\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 0.5 \t inf \t nan\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 1 \t inf \t nan\n",
      "lamb\ttau\tlayer_loss\t\tfinal_loss\n",
      "0.01 \t 5 \t inf \t nan\n"
     ]
    }
   ],
   "source": [
    "def torch_DGD(W, A, b, max_iter, step_size,tau):\n",
    "    (batch_size, num_of_nodes, _, dim) = A.shape\n",
    "    init_x = torch.zeros((batch_size, num_of_nodes, dim))\n",
    "    \n",
    "    \n",
    "    (batch_size, num_of_nodes, dim) = init_x.shape\n",
    "    I = torch.unsqueeze(torch.eye(num_of_nodes), axis = 0)\n",
    "    I = I.repeat(batch_size, 1, 1)\n",
    "    \n",
    "    W_hat = (W + I)/2\n",
    "    \n",
    "    #initialization\n",
    "    k = 1\n",
    "    x_0 = init_x\n",
    "    x_12 = W @ x_0 - step_size * tgrad_qp(A, b, x_0)\n",
    "    x_1 = torch_soft(x_12, tau*step_size)\n",
    "    \n",
    "    x_hist = [init_x,x_1] #add for plot\n",
    "    while (k < max_iter):\n",
    "        \n",
    "        x_32 = W@x_1 -  step_size*tgrad_qp(A, b, x_1)\n",
    "        x_2 = torch_soft(x_32, tau * step_size)\n",
    "        \n",
    "        x_0 = x_1\n",
    "        x_1 = x_2\n",
    "        x_12 = x_32\n",
    "        \n",
    "        k = k + 1\n",
    "        \n",
    "        x_hist.append(x_2)\n",
    "        \n",
    "    return x_2,x_hist\n",
    "lams = [5e-4,7e-4,1e-3, 2e-3,5e-3,1e-2]\n",
    "taus = [1e-2, 5e-2,1e-1,5e-1, 1, 5]\n",
    "best_error = 100\n",
    "best_par = {}\n",
    "for lam in lams:\n",
    "    for tau in taus:\n",
    "        for iter, (W, A, y, x_true,pyg_data) in enumerate(val_loader):\n",
    "            original,origin_hist = torch_DGD(W, A, y, 100, lam, tau)\n",
    "            loss2 = opt_distance(original.detach().numpy(), x_true.numpy())\n",
    "            loss1 = opt_distance(origin_hist[num_layers].detach().numpy(),x_true.numpy())\n",
    "            \n",
    "            print(\"lamb\\ttau\\tlayer_loss\\t\\tfinal_loss\")\n",
    "            print(lam,'\\t', tau, '\\t',loss1,'\\t',loss2)\n",
    "            if loss2 < best_error:\n",
    "                best_par['lam'] = lam\n",
    "                best_par['tau'] = tau\n",
    "                best_error = loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lam': 0.002, 'tau': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(best_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter, (W, A, y, x_true,pyg_data) in enumerate(test_loader):\n",
    "    _,pred_PGEXTRA,pred_PGEXTRA_hist = model_PGEXTRA(W, A, y, pyg_data,num_layers)\n",
    "    _,pred_DGD,pred_DGD_hist = model_DGD(W, A, y, pyg_data,num_layers)\n",
    "    #_,pred_NIDS,pred_NIDS_hist = model_NIDS(W, A, y, pyg_data,num_layers)\n",
    "    \n",
    "    original_PGEXTRA,original_PGEXTRA_hist = torch_PGEXTRA(W, A, y, 200,0.002,0.05 )\n",
    "    original_DGD, original_DGD_hist = torch_DGD(W, A, y, 200,0.002,0.1)\n",
    "    #original_NIDS, original_NIDS_hist = torch_NIDS(W, A, y, 200,0.005,0.01)\n",
    "\n",
    "\n",
    "origin_PGEXTRA_error = hist_nmse(original_PGEXTRA_hist,x_true)\n",
    "origin_DGD_error = hist_nmse(original_DGD_hist,x_true)\n",
    "#origin_NIDS_error = hist_nmse(original_NIDS_hist,x_true)\n",
    "pred_PGEXTRA_error = hist_nmse(pred_PGEXTRA_hist,x_true)\n",
    "pred_DGD_error = hist_nmse(pred_DGD_hist,x_true)\n",
    "#pred_NIDS_error = hist_nmse(pred_NIDS_hist,x_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_name = \"D\"+str(n)+\"M\"+str(m)+\"NO\"+str(nnz)\n",
    "writer_error=pd.ExcelWriter(\"./error_fig/noise3/\"+figure_name+\".xls\")\n",
    "df_error= pd.DataFrame({'PG-EXTRA':origin_PGEXTRA_error,'DGD':origin_DGD_error})\n",
    "df_error.to_excel(writer_error,sheet_name='Origin')\n",
    "    \n",
    "df_feasibility= pd.DataFrame({'PG-EXTRA':pred_PGEXTRA_error,'DGD':pred_DGD_error})\n",
    "df_feasibility.to_excel(writer_error,sheet_name='GNN')\n",
    "writer_error.save()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEOCAYAAACetPCkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd4FNXXwPHvzW56QkI6KZCE0BFQihQRFFFQAWnSIdJBBAErqIANRRErINLLjyoICIiiiLwCQkC6dEhIAqSR3rPz/rHJbELaJmwq9/M8+7B7d3bmJsaczC3nCEVRkCRJkiRjmVV0ByRJkqSqRQYOSZIkqURk4JAkSZJKRAYOSZIkqURk4JAkSZJKRAYOSZIkqURk4JAkSZJKRAYOSZIkqURk4JAkSZJKRFvRHSgLLi4uiq+vb0V3Q5IkqUo5fvx4lKIorsUdVy0Dh6+vL0FBQRXdDUmSpCpFCBFszHFyqEqSJEkqERk4JEmSpBKRgUOSJEkqERk4JEmSpBKplpPjkiSBTqcjNDSUpKSkiu6KVInY2tri7e2NmVnp7xuqTOAQQnQDvgI0wFJFUT6p4C5JUqUWFRWFEIIGDRrc1y8JqfrQ6XSEhYURFRWFm5tbqc9TJX6ahBAa4DugO9AYGCSEaGzq68RG3mXFom3s3brf1KeWpHIXGxuLu7u7DBqSyszMDHd3d+Li4u7vPCbqT1lrA1xRFOWaoijpwAaglykvsGPDb7SZd4D5lzO5uXgJWYmJpjy9JJW7rKwszM3NK7obUiVjbm5OZmbmfZ2jqgQOL+Bmrteh2W0qIcRYIUSQECIoMjKyxBd4uFVDFMA5NZ55zQdwZvBwdCkp99VpSapoQoiK7oJUyZjiZ6KqBI5iKYqyRFGUVoqitHJ1LXbHfD4+AT7YZaURXMODdI05Z2OziN28uQx6KkmSVLVVlcARBvjkeu2d3WZSDd1s1eenXeqS8Iec65CksuLr64u1tTV2dna4u7sTGBhIYjkMEQshsLW1xc7ODmdnZ7p06cLGjRvzHffbb7/xxBNPYG9vj7OzMy1atODTTz8lNTUVgNmzZ2Nubo69vT329vbUr1+fSZMmcevWrTL/GipaVQkcx4B6Qgg/IYQFMBDYYeqLDHiyqfr8tEtdko4dIytRLmWUpLKyc+dOEhMTOXHiBEFBQXz44Yd53lcUBZ1OZ/Lrnjp1isTERC5evEhgYCCTJk1izpw56vubN2+mX79+DB48mODgYKKjo9m4cSOhoaHcvGkYNR8wYAAJCQnExMSwbds2bt++TcuWLat98KgSgUNRlExgErAX+A/YpCjKOVNfp1MTT/X5JUcfUtGQdPiQqS8jSdI9vLy86N69O2fPnqVz587MnDmTDh06YGNjw7Vr1wgPD6dnz544OTkREBDADz/8oH722WefZfr06errgQMHMnLkSKOu6+LiwrBhw1i0aBFz584lOjoaRVGYNm0a7733HmPGjMHJyQmABg0a8M0331CvXr185zE3N6dJkyZs3LgRV1dX5s+ff5/fkcqtyuzjUBRlN7C7LK9R09aCOvZaghMy0ZlpOO/si932XdTo2rUsLytJD7ybN2+ye/du+vTpw8GDB1mzZg179uyhQYMGKIpCly5daNq0KeHh4Vy4cIGuXbtSt25dnnzySZYvX06zZs147rnnuHXrFkePHuXUqVMlun6vXr3IzMzk6NGj+Pn5ERoaSt++fUv8dWg0Gnr16sXevXtL/NmqpMoEjvLyxEPerDx0A4B/XevT8O8DFdshSTKR/xo2KrdrNbrwn1HHvfDCC2i1WhwcHHjuueeYMWMG3bt3JzAwkCZNmgD6oPL333+za9curKysaNGiBaNHj2b16tU8+eSTeHh4sGjRIkaMGEFKSgo//fQT9vb2Jeqvubk5Li4uxMTEqJ/18PBQ3x84cCC//PIL6enpfP/99wwbNqzQc3l6ehITE1Oi61c1VWKoqjw9Xt9FfR7k3hDblASigkMrsEeSVH399NNPxMbGEhwczMKFC7G2tgbAx8ewFiY8PBwnJ6c8waBOnTqEhRnWx/To0YOsrCwaNGjAY489prY3adIEOzs77OzsOHjwYKH9yMjIIDIyEicnJ5ydnQHyzFNs2LCB2NhYHnnkEbKysor8msLCwtThrepKBo57tPV3xkKr/7YE1/Ag0tqB9d9squBeSdKDJfdeg5y/4BMSEtS2kJAQvLwMW7lmzpxJo0aNuHXrFuvXr1fbz507R2JiIomJiXTs2LHQ623fvh2tVkubNm1o0KABXl5ebN26tcT91ul07Ny5s8hrVQdyqOoeNhZaHvVz4uDlKACOuTXkWGQmL4Xdws6rVgX3TpJKz9jho8rGx8eH9u3b8/bbb/P5559z6dIlli1bxrp16wD466+/WLFiBadOneLatWv07t2bxx9/PE9gKUxMTAx79uxh2rRpvPnmm+rdxvz58xkzZgw1atSgX79+ODo6cuXKFe7cuVPgeTIzM7l8+TKzZ8/m9u3bTJs2zXTfgEpI3nEUoFN9/QZCm4xUfqvdmos1fVi76McK7pUkPbjWr1/PjRs38PT0pHfv3syZM4ennnqK+Ph4hg8fzrfffouXlxcdO3Zk1KhRvPTSSyiKUuj5mjdvjp2dHQEBASxdupQFCxbw/vvvq+8PGDCATZs2sXbtWnx8fHBxceHFF19k7Nix9O/fXz1u48aN2NnZ4eDgQM+ePXF2dub48eN4enoWdNlqQxT1za2qWrVqpdxPzfHIhDQiElL5/ddjfHEhDYBaSVH8Pr0zNr51TNVNSSpT//33H40ald+EuFR1FPazIYQ4rihKq+I+L+84CuBqb0kTTwdeGtgJe50+cNyydWHjQpmCRJIkSQaOIthbmTOsqbP6emViTZL+/bcCeyRJklTxZOAoxth+7bBW9CmIg2t4sPWL1SjFLMeTJEmqzmTgKERmdDRXXp7Mqknv45NkSNO+xqY+dzfK5bmSJD24ZOAohLC0IuWP31nm8BCX7AzLcC/VrM0vq3eQWc13hkqSJBVGBo5CaOxssbC2olPYyXzvrfduR0Q1T2ImSZJUGBk4iqB1d+fJmyfytZ9xqcuh/SdIOnKkAnolSZJUsWTgKILW3Z0Gd0OolRiV770N9btwa8ZMWa9DkqQHjgwcRdC6uSKAzqH5l+AGeTTidIqWiM8/K/+OSZIkVSAZOIpg7q5Pq5w7cJjlqvO+ulE3YjdsJOnw4fLumiRVeUWVji2ubOu9bty4gRBCzYSb88gpCdu3b1/GjBmT5zO9e/dm0qRJfPzxx+rxVlZWaDQa9XVOavfc5Wa9vLyYNm1agVlyAwMD0Wq1sgLgg8y65SMA1E6MICA5AgCdAjmJO0+61eekSwDhM2eSFR9fUd2UpCqroNKxxpZtLUhsbKyaDTcxMZEBAwYA8N1337F161b2798P6HNMnThxgk8++YQZM2aoxy9evJh27dqpr8+dMxQazSk3e+DAATZu3Mjy5cvzXDspKYkff/wRBwcH1q5da+LvVOUiA0cRbNu1Q9jYANDp2lG13cPeSn2+qnE3MsJvcWvWrCKTqkmSVLic0rFnzpwpcdlWY3h4eKgZb0NCQpg8eTLff/89dnZ2JT5XQEAAHTp04OTJvCsuf/zxRxwdHXnvvfdYtWpVqfpZVcjAUQQzS0vssvPqdww3lKKMTEzDPHvM6oKTL0fdG5Gw5xfifpQZdCWpNHJKx9rY2JS6bGtxAgMDqVu3Lo888gjdunWjW7dupTrPhQsXOHjwIAEBAXnaV61axaBBgxg4cCAXLlzg+PHjpuh2pSTrcRTD/qkuJOzdi3taPI3Mkrht5cgzTTzI0ilsPq6vDLiqcXda3bnA7Y8+xvrhh7GsW7eCey1J+fm+tavcrnXjk+eMOu7e0rGDBg1iy5YtpS7b6uLikuf14cOH82SB7dixI7/++itDhw4t4VeEWv0vOTmZgQMHMnHiRPW9kJAQ9u/fz/z583F3d6dLly6sXr2ali1blvg6VYG84yiGXadOeM77lPp//x9L3+jBsZlP8UnfZrzerQHW5hoArjt4stf3UZSUFMKmTUeXklLBvZakquHe0rE5v/iLK9uaewI8JCREPTYqKorY2Fj1kTtoXL58mc8//5yJEycyffp0MjIyStTXEydOkJiYyMaNG/nnn39ISjIsxV+zZg2NGjWiRYsWAAwZMoT//e9/Jb5GVSEDRzE0NWrg0LMnGkdHvByt0Wr03zI3eyvGdfJXj1vZ+FnizW1Iu3iR27PnyPkOSSoFY8u25p4Ar127drHnVRSF0aNH8+qrr/LNN99ga2vLp59+WuL+CSF48cUXadeuXZ7CT6tXr+batWt4eHjg4eHBtGnTiIqKYvfu3SW+RlUgh6ruw/hOdfnxRCg3Y1JIsLBhZePuTD71I3Hbt2PVvBlOgwdXdBclSWXs8FFFMjMzK3HZVmMsWrSIqKgoZsyYgZmZGcuWLaNt27b069ePhg0blvh8b731Fm3btuWtt97i+vXrXL16lX///RdXV1f1mOnTp7N69Wp69epV6n5XVvKOo4QURSHpn6Mk7NuHucaM955vor73i19bLjl6A3Bn7icky9odklRixpZtLYijo2OeYawvvviCkJAQZsyYwbJly7CwsACgcePGTJ8+nTFjxpRqdOChhx7i8ccf57PPPmPVqlX06tWLhx56SL3j8PDwYMqUKfz888/EVMOEqLJ0bAmk37zJzanT2BtrzmGfFvzn24z/e/spJqw7wf6L+tTrDVMi+XzvPDQoaN3c8N28CXN3d5P3RZKKI0vHSoWRpWPLkdbNjcw7t1nX8GkOujchKiWLPw6cYlaPJlhkz31csHbl5yZPAZAZEUHohInokpMrstuSJEkmJQNHCZhZWuLz5Zd0iLqotv28/f+o42zDxCcMS3BXNnyGcHv9WGfq+fOEvfEGik5X7v2VJEkqCzJwlJBNy5a8MN4wzvqPxoWEP/5gYucAGnrYA5CaBQt7vYYO/SbBxH2/y/odkiRVGzJwlEKbji2wR1+HPNrakaPfrcJcyeLz/s3RZO8oP56o4c9BU9XPxCxbzt3Nmyukv5IkSaYkA0cpaDVmPNbAMOF9ON2W6BUraerlwPhcezu+y/IhoYthCeTtOe/LTLqSJFV5MnCU0hNNvdTnQe4NiPruO9JDQpjcpR4BbvrEacnpWXzVrA8WOasXMjMJnfQKKWfPFXRKSZKkKkEGjlLq1MCw0eecsx/JmQp3PvkUS62Gz/o1U+t2/H39Lkdefh9t9pJcXVISN8eOJe369YrotiRJ0n2TgaOU3GtY0cBdPxmeaablQs3apJ49S+bduzxcuyajOxqGrD45GIbFVwsxc3AAICsmhpBRo8i4j52wkiRJFUUGjvvQ2q+m+vz6swOp+8setDX1bdO61sfPxRaAhLRMZv2biPeihQhrawAyw28RMmoUWbGx5d9xSZKk+1ApAocQor8Q4pwQQieEaHXPe28LIa4IIS4KIZ6pqD4WpLWvvsiMxkyQ2rg5ZtlFnwCszDXM69dMrRb458VI9ihueH/9FWj1KcLSr1zl5rjx6HJl2ZSkB8mGDRt49NFHsbW1xc3NjUcffZSFCxeiKAqBgYEIITh61FBE7cqVKwhhqN/cuXNnrKys8lQG3LdvH76+voVec/bs2Zibm2NnZ4ejoyPt27fncDktWgkMDMTCwgJ7e3vs7e1p2rQpb7/9NnFxcXmOu3XrFmPGjMHT0xM7Ozv8/f0JDAzkwoULQP5Sue7u7jz//PP89ttv5fJ1VIrAAZwF+gB/5W4UQjQGBgJNgG7AQiGEpvy7V7DH67mybvSjnJ71NB+80DTf+619nRjRzld9/f7OcyQ1b43n3LlqW8qpU9wcP0HuLpceOPPnz2fKlCm8/vrr3L59mzt37rB48WL+/vtv0tPTAXBycuKdd94p8jy2trZ88MEHJbr2gAEDSExMJDIykscee4w+ffoUmLMqMzOzROc1xhtvvEFCQgKRkZGsWLGCI0eO0KFDBzVNe3R0NO3btyc5OZmDBw+SkJDAiRMn6NSpU77AkFMq99SpU3Tt2pXevXuzcuVKk/f5XpUicCiK8p+iKBcLeKsXsEFRlDRFUa4DV4A25du7wtW0taBDgAu2lvmTDCvZefjf6NaA2k76O5H41ExmbD1Djeefwz3X/wzJx45xc+LL6FJTy6fjklTB4uLieO+991i4cCH9+vXD3t4eIQQPP/ww69atw9LSEoARI0Zw+vRpDhw4UOi5Jk+ezPr167l69WqJ+2Fubs6IESO4ffs20dHRrFy5kg4dOjB16lScnZ2ZPXs2Op2ODz/8kDp16uDm5sbw4cPVO4SNGzfi5+dHfHw8AHv27MHDw4PIyMhir21lZUXr1q3ZsWMH0dHRrFixAoAFCxZQo0YN1qxZQ926dRFC4OjoyEsvvcQrr7xS4LlykirOnj2bN998E10ZZ6qoFIGjCF5A7ur0odlt+QghxgohgoQQQcb8Rysrqf/9R+irU7nerz9Kejo2Flo+6fuQ+v7vFyLYHBSK09AhuL3+utqefOQIoS9PQpeWVhHdlqRydfjwYdLS0opNOW5jY8OMGTOYOXNmocd4eXkxZswYZs2aVeJ+pKWlsXLlSjULL8A///yDv78/d+7cYebMmaxcuZKVK1eyf/9+rl27RmJiIpMmTQL0dy7t27dn8uTJREdHM2rUKJYuXZonvXpx7O3t6dq1KwcPHgT0Q229e/fGzKzkv5779OlDREQEFy8W9He46ZRb4BBC7BNCnC3gYZJk9YqiLFEUpZWiKK1K8h/NlDLv3uXGkKEk/PILaRcvEvX9EgDa13UhsL2vetycnee4GZOM86iRuE417C5P+vtvQidPRpd9my5Jphb5zbf817CRUY9b776X7/O33n2vyM9EfvOtUf2IiorCxcUFrdZwt96+fXscHR2xtrbmr78Mo9bjxo0jJCSEPXv2FHq+t99+m507d3LunHF7pDZt2oSjoyM+Pj4cP36cbdu2qe95enryyiuvoNVqsba2Zt26dUybNg1/f3/s7OyYO3cuGzZsUIexvvvuO/744w86d+5Mjx49eP75543qQ26enp5q+vWoqKg8pXN37NiBo6Mj9vb2PP3008WeByjzVO7lFjgURXlKUZSmBTy2F/GxMMAn12vv7LZKJTk9kz8vRvD5kTs4TZmitkctWULG7dsAvNmtIf7Zq6yS0rOYvukUWToFl3FjcXllkvqZpAN/ETZ5irzzkKo1Z2dnoqKi8swhHDp0iNjYWJydnfMMtVhaWvLuu+/y7rvvFno+V1dXJk2axHvv5Q1269atUyeQu3fvrra/+OKLxMbGEhERwR9//JGnNriPj0+ec4SHh1OnTh31dZ06dcjMzFQLSzk6OtK/f3/Onj3L9OnT1eM+/vhj9drjx48v8vsRFhaGk5OT+r3JXTq3Z8+exMbGsmDBAnXup6jzAOq5ykplH6raAQwUQlgKIfyAesDRYj5T7p796iCBK46x+MBVbj7+LFYPZQ9NZWQQt30HANYWGr4Y0ELNZXX0RgxLD14DwGXiRJzHj1PPl/jnn4ROkBPmUvXVrl07LC0t2b69qL8bDV566SViY2OLLCn7+uuvs3//fo4fP662DRkyRC0xW9QdS265V22B/q/44OBg9XVISAharRb37E29J0+eZPny5QwaNIjJkyerx82YMUO99uLFiwu9XmJiIvv27aNjx44AdOnShZ9++qlU8xTbtm3Dzc2NBg0alPizJVEpAocQorcQIhRoB+wSQuwFUBTlHLAJOA/8ArysKEpWxfW0YDnLcgEOXbuL07Ch6uu4rVvV1RotfBx5+YkA9b3P9l4k6EYMQghcp0zBeZwheCQdOkzImLFkJSaWw1cgPShcX5lEowv/GfWo9cH7+T5f64P3i/yMa66756I4Ojoya9YsJk6cyJYtW0hISECn03Hy5El1dVFuWq2WOXPmFFkn3NHRkenTpzNv3jzjvyFGGDRoEAsWLOD69eskJiYyY8YMBgwYgFarJTU1laFDh/Lxxx+zYsUKwsLCWLhwoVHnTUtL4/jx47zwwgvUrFmTl156CYBp06Zx9+5dhg0bxtWrV1EUhYSEBE6ePFnoue7cucO3337LnDlzmDt3bqnmR0qiUgQORVG2KYrirSiKpaIo7oqiPJPrvY8URamrKEoDRVGM+5OhnHUIcFGf/9+VKOy7dsXMVj8slR4cTMq/hv/grzwZQAsfRwAydQqT/vcv0YlpCCFwm/oqrq8ahrpSjh8n5KWRcpOgVC298cYbfPHFF8ybNw93d3fc3d0ZN24cn376Ke3bt893/KBBg6hVq1aR55wyZQoajWlX7I8cOZJhw4bx+OOP4+fnh5WVFd988w2gn1vx8fFhwoQJWFpasnbtWt555x0uX75c6PnmzZuHvb09zs7ODB8+nJYtW3Lo0CFss39nuLi4cOTIEaysrHjsscewt7enRYsWJCQksGjRojzncnR0xNbWloceeojdu3ezefNmRo4cadKvvyCydKwJRCSk0uaj3wEw1whOzXqauA/mELt5CwDWDz+Mzw8/oLHT/2CExabw3NcHiU3WL9ntWM+FlS+1UYexoleuJOITw19Wlg0aUHv5MrTOzuX2NUlVnywdKxVGlo6tBNzsrdQiThlZCvsvROLY31DsKeXff7k5bhxK9kSgl6M1Cwa0UN8/eDmKL/ddUl87BwbiMduwtDDt4kWChw0n405EWX8pkiRJxZKBw0SeaWJYPrf7zC2smzXD7c031baU48dJzF6nDfBEAzcm5Zrv+OaPK2z7N1R9XXPgQGp9MheyxyrTr10jeMgQ0m/cKMOvQpIkqXgycJjIc80MY69/XIggOT0T55cCqTk0e6JcCNIuXsrzmald69OxnmF+5M0tZzh63bD+2vGFF/D6Yr6a2yojNJQbg4fIeh6SJFUoGThMpL67vVrAKSUji/0X9LvXaw4cgMfsWdT76wAuuZbcgj454ndDHqG+u/5z6Vk6xq4J4nqUYVVJjW7d8P72G4SVFZCdkn34cJIOHSqPL0uSJCkfGThM6LmHDHcdu86EA2AZEEDNgQPRFrKbvYaVOctGtMbFzgKA2OQMAlccJTLBsAHQvnNnaq9Yrtbz0CUnEzJuPPG7d5fVlyJJklQoGThM6Plcw1UHLkaSmmHclhMfJxt+GN4KS63+P0dwdDKBK46SkJqhHmPz8MP4rluLNicVQUYGYdNfI2bNWtN9AZIkSUaQgcOE6rnb07+lNx/0asJfbzyBlXn+9eS69HTidu3K1/5w7Zp8M+hhteTsufB4xq89TlqmIfhYBgTgu/5/WNStq29QFO589BERXywoMCW0JElSWZCBw8Q+69+cYe18cbazzPdeVkICN0ePIXz6a6ScPp3v/aebePBxb0Mm3b+vRDNt0yl0OkNQMK9VC991a7FuYVjOG71kCeGvvyGTI0qSVC5k4ChHEfPnk5xdzSz2x4Jz7gxsU5vXnq6vvt51+hZzdp7Lc0ehcXSk9orl2HXqpLbF//wzISNHknn3bhn1XpIkSU8GjnLk8Nxz6vP4PXsKzYD78hMBjGhnyMa56nAwC//MW6TGzNoa7+++xXHgALUtJeg4wQMHkZ4rIZskVWaVsXRscWVbC5LTj5xsuHZ2dvTo0QOAnTt34uHhkSfV+fbt2/Hy8iI4ODjPZ4QQ2Nraqq8PHjyolpu1s7PDycmJrl27FtiXP//8EyFEkfm8TEUGjjKiKApnQuNYc/iG2mbdsiXm3t4A6OLjSdy/v8DPCiF4r0eTPHtDPtt7kQ1HQ/Iep9XiMWtWnoJQ6cHB3Bg4iOQT/5rui5GkMlAZS8eWpGzrvb799ls1G25iYiI7d+4EoEePHjz55JNMza69Exsby4QJE1i0aBF16tTJ8xmAU6dOqa9zMua+8cYbJCYmEhYWhpeXF6NGjcp3/VWrVuHk5MTq1atL9L0oDRk4ykBqRhZd5h+gx7f/x3s7zhERry8JK8zMcMhV8Sx6+Qq1xOy9NGaCL15sTocAQ36qGdvO8Nv5O3mOE0LgPGokXl9+icgut5l19y4hgYHE//KLqb80STKJylo6tjRlW43x9ddfs2fPHvbu3cvUqVPp1KkTPXv2LPF5rK2tefHFF/Nlyk1KSmLLli189913XL58mbLO1ScDRxmwMtfgVkP/g68o+hQkORx691Z3gqeePk3Egi8LPY+lVsPioS1p6lUDAJ0Ck/53giPXovMdW6PbM9RZtRJNdgEXJT2dsFenEr10qVxxJVU6lbV07P2UbS2Ki4sLX331FUOGDOHnn3/m66+/LtV5kpKSWL9+PQEBAXnat27dip2dHf379+eZZ55h1apVpuh2oWTgKCPPN/NUn/94wlC00MLbC7dcqdNjli8vcIVVDnsrc1YEtqGOsw0AaZk6Rq08xomQ/JPg1i1a4LtxAxZ+fmpbxOfzufXOOyhyxZUELPjtEr5v7TLq8fbW/D+Xb289XeRnFvx2qYCr5ldZS8feT9nWyZMn4+joqD7urVjYtm1b4uLiePrpp0tUkxzg888/V/vxf//3f6xZsybP+6tWrWLAgAFoNBoGDx7Mhg0byChkNMMUZOAoI883q4VF9oa+M2FxnA2LU99zGjkS28c7qq9jVhU9Julqb8makY/iZq+/i0lKzyJw+VHOhcflO9bCxwff9f/DppUhM3Lcj1sJHjmSzDKuQyxJxqqspWONKds6fvx49Zwff/yxeuzXX39NbGys+rh33mXs2LEMHz6c3bt355mMN8Zrr71GbGwsN27cwNramosXL6rv3bx5k/379zNkyBAAevXqRWpqKrsK2C9mKjJwlBFHGwuebWr4y2V9roltYWaG26uvqq/j9+4lI6LolOm1nW1YN/pRnGz1qUniUzMZtuwol+8k5DtW4+iIz/JleeZTUoKOc6Nff1Jz/cBJUkWprKVjjSnbunjxYvWcM2bMMKr/y5Yt4+bNmyxcuJCPP/6Y0aNHF1s/vCC1a9fmq6++YsqUKaSkpACwZs0adDodPXr0wMPDA39/f1JTU8t0uEpb/CFSaQ1qU5ufTupzVm0/Gc6MZxtha6n/lls1box1y5akHD+7mlaXAAAgAElEQVQOWVkkHzmCQzGTZfXc7Vkzqg2DlhwhPjWTmKR0hiz9h03j2uHrYpvnWDMLC2p9MhfL+vWI+Hw+KAoZ4eHcGDQYr8/mYd+lS9l80VKlNrVrfaZ2rV/8gYWY26cZc/s0u+9+5C4dqygKzzzzDLa2tpw+fbrI0rG5a3oXdM6c0rH29val6te0adNYu3Ytw4YN4/3338ff35/ExMQiy7YWJzw8nNdff53t27djaWnJ+PHjWb9+PR999BFz5swp8fm6du2Kp6cnS5YsYcqUKaxatYpZs2Yxfvx49ZijR4/Sv39/oqOjcS6DAnDyjqMMtfFzwt9V/ws9MS2T/Rfz3lU4jxqJ04gR1N37S7FBI0cTTwdWjWyDrYU+nUlEQhqDfzhCSHRyvmP1K65G4b3wO8xs9HMkSnIyoZNeIer7JXLSXKpQlbF0bEnKtt5r0qRJefZk5Ax/TZw4kYEDB6pLa4UQ/PDDD3z55ZdGz8nc6/XXX2fevHkcOHCA4OBgXn75ZTw8PNRHz549CQgIYP369aU6f3Fk6dgytuC3S3z1u77+cI/mnnwz6GGTnPfItWgCVxwlNUN/S+3laM36MW2pnT2Jfq+0y5e5OWEiGaGGYlE1evSg1ocfYGaZPz2KVPXJ0rFSYWTp2Eoud2XA/Rci8iQtvB9t/Z3zZNQNi01hUCF3HgCW9erhu3kTNq1bq23xO3fqS9IWM78iSZKUmwwcZaxRLXt8nKwB/XBV0I2ic0llhIWhS0016twd67mWKHhoa9ak9rKlOL74otqWevo0N/q/SMqZs0ZdU5IkSQaOMiaEYFQHP6Z1rc+vUx+nfd2CJ6oy797lziefcrVbd+6uW2f0+R+vX7LgISws8JgzG/d33oHsseDMO3cIHjKE2J9+KuFXJ0nSg0gGjnIQ2MGPyV3qUd/dPk+CttwSfvmFmJUrUTIyiFq4iIw7+tQi6cHBZISHF3n+EgcPIXAaOgSfJd9jVkO/K11JT+fWW29z+8OPCk2DIkmSBDJwVBqO/fph4e8PgC4piTsffkjcz7u42q07V55+hrhi1rsXFDwGLjlcaPAAsOvQAb/Nm7CsZ0hfcHftWkJeGklmdP60JpIkSSADR6UhzM2pNWe2+jrht32Ev/aaPtlVZia3Zs0mMzKyyHPcGzzC41KLDR4WdepQZ/0G7HOlU0gOCuJ6335y3qMaqI6rJqX7Y4qfCRk4ytmtuBT+ulRwALBp3RrH/v0KfM/zs3lojchv83h9V5aOKFnw0NjZ4vXVl7hOnQrZQ2mZt2/r5z22biv2mlLlZGVlRXR0tAwekiondbyVldV9nUfu4ygn8akZ9F14iMsRiVibazg5qyuW2gJqkicnc3P8BLVSIIDzuHG4TX0137FFOXg5ktGrgkjL1O/z8HSwYsPYdoXu88iR+NdfhL32Orr4eLWt5pAhuL/1JsLcvER9kCpWRkYGoaGhpBq5Sk96MFhZWeHt7Y15Af8/G7uPQwaOctT5s/3cyP7Lf+VLrencwK3A43QpKYS/8SYJv/+Oy/hxuBaRZqEopQ0e6cHBhE56hbTLl9U261Yt8f7yS7QuLqXqiyRJlZ/cAFgJPdXIXX2+NVeq9XuZWVvj/c3XNDxzutRBA/T7PEo6bAX6eQ/fDeuxf+YZtS0l6DjX+/Un5cyZUvdHkqTqQQaOctS3pbf6fO+528SnFr3sVdxHzp0cBQWPAUsOcy0yscjPmdna4vXlAlynTcs77zF4CHc3b77vfkmSVHXJwFGOGtWqQeNa+n0TaZk6dp2+Vcwn9CK+WMCNAQO5+uxzJB87VuLr3hs8bsWl8uL3hzkfHl/k54QQuIwdk3e/R0YGt999j/CZM43e4S5JUvUiA0c565frrmPbv4UPV+WWfuMGKadOkX7tWqn3V3Ss58rywNZYm+vvYqIS0xm45HCBlQTvZdexI35bNmPZoIHaFvfjVm4MHkx6rqSJkiQ9GIwKHEIIp2Le1wghHiltJ4QQnwkhLgghTgshtgkhHHO997YQ4ooQ4qIQ4pmizlMV9GjumTPyw/Hgu8QmF1/MxayGobZAVlzRdwlF6RDgwtrRbbC30tcEiU/NZOjSfzh0JarYz1rUro3vhvU49DKkf087/x/X+/Yj8cCBUvdJkqSqx9g7jkghhLoESAjxrxDCO9f7LkDJx1AMfgOaKorSDLgEvJ19ncbAQKAJ0A1YKIS4/4H/CuRqb0kLH31czNIpHChkT0duGvsa6nNdQukDB0DLOk6sH9NWrSSYnJ5F4Mpj7Dt/p9jPmllbU+uTT/CY9R5kL+XTxcVxc/wEIr/+BiXLNJl/JUmq3IwNHPcmWAoALIo5xmiKovyqKEpO8eEjQE5Q6gVsUBQlTVGU68AVoE1pr1NZdGloWIb7+3/FpzTXOBgCR1Z8/lKxJdXUy4FN49rhUUO/CSg9U8e4tcfZfrL4oTMhBDUHDcJ37Rq0Htkp4xWFqIULuTl+Apl3ix/6kiSpajPlHIepNoSMBHKKA3sBN3O9F5rdVqU92dCwLPfPixFkZhVe3xjALFcZzKz4OJP0IcDNjs3j21Ene09Hlk7h1Y0nWXP4hlGft27eHL+tP2LTrq3alnTwIDf69iPlbOmqmkmSVDWU2+S4EGKfEOJsAY9euY6ZCWQCxucVN3x2rBAiSAgRFFlMTqeK1qiWPW18nQhs78t3Qx4pNGNuDk2NXENVJrjjyOHjZMPmce2o724H6NNivbv9HF/uu2RUmgqtkxO1ly7FeexYtS0jPJzgQYPkkl1Jqsa0Rh6nkPeO4t7XxZ9AUZ4q6n0hRCDwPNBFMfzWCgN8ch3mnd1W0PmXAEtAv3O8JH0rb0IINo1vZ/Txee44EkwXOADcalixcWw7Xlp5jJM3YwH4ct9lYpLSmd2jCWZmRQc1odHgNm0q1s2bEf7mW+gSE9UluyknT+Lx7ruY3WdeHEmSKpeSzHEcEUJcEkJcAmyBP3K9PnQ/nRBCdAPeAHoqipJ7W/MOYKAQwlII4QfUA44WdI7qTFPDQX1uqqGq3GraWrBu9KN0rGdIJ7L6cDCTN/xLembRw2g57Lt0we/HLVjWr6+2ySW7klQ9GXvHMadMewHfApbAb9nDNkcURRmvKMo5IcQm4Dz6IayXFUV54JbuaHItx80Zqsq5KStumMtYtpZalo1ozfTNp9h5Sl846ufTt4hLyWDx0JbYWhb/o2JRpw6+Gzdwa9Ys4nfsBAxLdr3mfYpdp04m6askSRVLJjmsBFIzskjL0OFgU3D22Yw7EVzJ9UtX6+5OVkwMfj9tw7JuXZP2RadTmLPzHKsOB6ttzX0cWRHYWl3CWxxFUbi7fj135n4COdUEhcBlwgRcXp5oklQqkiSZXrkkORRCdBRC9C1ug6BUsD8vRjBwyWGazfmVxX9dLfQ4bU1HPD//HO/Fi0CrJfPOHZSMjDKp0mdmJpjdswnTuhqGnE7djKX/4kOExaYYdQ4hBE6DB+O7ZnX+Jbtjx5EZE2PyfkuSVH6M3Tk+SQjxzj1t24EDwGbgshCiYRn0r1pLzdBx5FoM6Zk69p69XeiyXGFhgcPzz5ESFASZmWp7VkzZ7JkQQjC5Sz0+eKGpusv9amQS/RYd4kqE8ZPz1i1a6Jfsts21ZPfvv7neuw/JJ/41dbclSSonxt5xDAdCcl5kL6F9FhgGtAYuAzNM3rtqrn2AM7YW+mGba1FJrPsnpMjj04Pzvp8ZU7Z1wYe1rcM3gx7GXKOPHrfiUum3+DD/GpHfKod+ye4PeZbsZt65Q/Dw4USvWCmr00lSFWRs4KgL5P4T8VngZ0VR1imKchyYCTxu6s5VdzWszJn0ZD319Re/XSoyd1V6SN7AUVZ3HLk938yTFYFtsMkOcLHJGQxZ+k+h5W8LIrRa3KZNxXvxIswcsleIZWYS8emnhE2eTFb8/aVRkSSpfBkbOKyB3P93twX+yvX6MlBwOTupSCMf81V3b8elZPBTIRlzFUUh/ebNPG1ZZXzHkeOxei758luNWnVMXX1lLPvOnfHf+iNWzZqpbQm/7eN6v/6k/vefSfssSVLZMTZwhALNAIQQNdEnHTyc631X8gYWyUiWWg1jOvqrr/ecvV3gcaHjJ6Ak563clxldfpPMzX0c2TSuHV6O1gBkZClM3vAvqw/fKNF5zL288F27hppDh6ptGSEh3BgwkLubNsmhK0mqAowNHBuBr4UQE4FV6PNH5d6I1wq4aOK+PTCebuKuTkIfuxFDVGJavmNSL1zI15ZVzquTAtzs2DKhHfXcDClK3tt+jgW/GZeiJIewsMDjnZl4LfgCMxv93ZaSns7t92Zx66230CUXXdpWkqSKZWzg+Aj9CqqP0GfGHaIoSu4lQIOAXSbu2wPDzd6KVnVqAqBT4LcCUpxnxcbma6uIZa21HKzZNK4dD9dWS6bw1e+XeXf7WbJ0JbtbqNG9O75btmBZzzDPE7d9BzcGDCDt2jWT9VmSJNMyKnAoipKqKEqgoig1FUVprCjKoXve76woyryy6eKDoVvTWurzgoarbNrkzyafVQb7OIyRk6KkU31XtW3tkRBeWX+C1IySbey39PfDd9NGHHr3VtvSLl/hRr/+xO2Sf4tIUmUkS8dWEt2aeqjPD12J4m5S3tVV7m+9icbJSa39DZAVF4eSa19HebKx0PLD8Fb0bO6ptu0+c5vAFUeJT80o0bnMrK3xnPsxtT76EGFpCYAuOZnw6a9x+/330aUXXyVRkqTyY1TKESHEEmNOpijK2OKPKntVLeVIjtGrgvBwsOT5Zp609nVCc09mWl1aGkKjIeXkSTSOjmicnNDUrGmyfFWlodMpfLDrPCv+vqG2NfSwZ9XINrjXKHlW3NQLFwidMoWMXHtWrJo2xevLL7HwrvKlWCSpUjM25YixgUOHfgPgNQqv9KcoivJkiXpZRqpq4KiqFEXh+7+u8ckewwS+l6M1a0a1wd/VrsTny0pM5NbMd0jYu1dtM3NwwPOTudg/8YRJ+ixJUn6mzlW1HagFZAGLgK6Kojxxz6NSBA2p/AkhGN+pLvP7N1fvksJiU+i3+LBa46MkNHZ2eH25APcZM0Crz8qri4sjdMJEIubPr7DhOUmS9IydHO8N+AJ/Ap8AYUKIeUKIekV9Tip7SmYmEfPnc2v2bDJuF7wHpLz0benN0hGtsDbX7zKPSUpn0JIj7L9YfF31ewkhcBo+TF/bvJZh4UD0D0sJCXyJjIiSn1OSJNMwenJcUZRbiqJ8hD79yIjsf88IIfYLIWSJNxNLSM3gQBFpPZT0dDLuRHB3/Qaif1hK7IaNXO/fn5QzZ8uxl/k90cCN/415lJrZKeJTMrIYsyqIH4+XrphTTqJE244d1bbkoCCu9+5D0pEjJumzJEklU+p6HEKIruhzVD0GuCiKUvIxiTJSlec4snQKE9cdZ//FSDKzdByZ0QU3+7xxOWrx90R++WWBnzf38aHu7l0I84Jre5SXq5GJDF92NE8q9re6N2Tc4/6lmsxXdDqilywh8utvQJe9hcjMDJeXJ+Iyfrys8SFJJlAm9TiEELWEEDOEENeANcARoEFlChpVncZMEJucQXqmDp0CvxSwp0Pr6lrAJ/Uybt4kI7xkOaTKQl1XO7ZObE9DD0P1wk/2XOCDn/9DV8KNggDCzAyX8eOpvXwZGmdnfaNOR9Q33xIyajSZkcYnXZQk6f4YW4/jeSHET8B1oDPwJuCjKMpbiqIUXoFIKpXnmxnG9H8+dSvf+5YBRVf9K4sCT6XhXsOKTePb0dbfUOdr+d/XeXXjSaNrmd/Ltm1b/LZtxaZ1a7Ut+cgRrvXuQ9KhQ0V8UpIkUzH2jmMH0Bx9ypGVgDnQXwgxOPejjPr4wOnWtJaauyooOCbfZkCLYsrFVpbAAfrU8StfasOzDxk2OO44Fc7IlcdITCvd6ihzNzdqr1iOy8SJ5HyjsqKiCBk1moivvpKrriSpjJVkqKoOMAdYW8hjjcl794BytbekhY8+F5ROId8kucbODq27u/raqkmTPO9XVCqSwliZa/hm0CMMa1tHbfu/K1EMXHKYyIT8CR2NIbRaXCe/oh+6cnHRNyoK0YsW61dd3ZGrriSprBi7HNfMiIecnTShLg0N5U3+uJD/l6BlrrsOlwnj8duxnTprVuO/ezcOPXuWSx9LQmMmeL9XE1572lDL/GxYPH0XHeJGVFKpz2vbrh3+27Zi085QnjY5KIjrL7xA4sGD99VnSZIKZuwcx+PGPMq6sw+SJ3IFjgOXIvPVI7fINc+RduUqVvXrY9O6NZb+fmqq8spGCMGkJ+vxad+HyMmmEhKTTL/FhzgTGlfq82pdXam9dCkuk18BM/2PdNbdu9wcM5aI+V/IoStJMjFjh6r+BPZn/3vvY3/24w/Tdu3B1rhWDTyycz3FpWRw9EbeFOqW/rkCx9WqtT5hQOvaLBnWCkut/scvKjGdgUsOc/By6VdGCY0G14kTqb1yRZ5VZ9E//EDw8BFk3Mq/yECSpNIxNnD4ALWz/8398EO/kzwVKLjmqVQqQog8dx3v/HSWpFyTyblXVqVXscAB8FRjd/435lEcrPX7TZLSsxi58hjbT97fj5Ftmzb4/bQN2w4d1LaUEye4/kJvEv78877OLUmSnrFzHGH3PoCHgd3AROB9oH6RJ5FKbHRHPzV9R9jdFE6FGrbL5F5ZlXr+PFmJiYA+g25VmRhuWceJLePbUctBf2eVkaUwZcNJlh68vyJOWmdnfH5Yguu0aZC9MTArLo7Q8RO48+k8lIySpX2XJCmvEtfjEEI8IoT4A9iKfniqnqIonyqKUrrlMVKh6rra8VHvpvi52LJtYgfa13VR39PWrKkGD42zM0pqKhdbteZi8xZc69GjorpcYvXc7dk6sT313Q1ZdD/c9R9zd5duo2AOYWaGy9gx1Fm9Cq2HYSlwzIoV3Bg6lPSbN++r35L0IDM6cAghfIQQa4FjQCzQRFGUVxRFiSqz3kn0ecSbPVM60tizRr73vL74AqfAQLy//QaNkxO6FH16D118fJUqflTLwZrN49rT2rem2vb9X9d4bfMpMrJKt1Ewh03Llvht24pdp05qW+qp01x/oTdxO3bc17kl6UFl7KqqT4CL6BMbPq4oSh9FUS6Xac8klZV5wSudrRrUx/2tN7F5+GGEmRkaJ8Mv3qwKqEd+PxxszFkz6lG6NjbsT9n6bxijVgXlmdspDW3NmngvWojb668b0rQnJRH+xpuEvfGGOswnSZJxjL3jeANQgERglhDi14IeZddNyRhaJ2f1eWZU5doEaAwrcw2LhjzCoDa11ba/LkUy+IcjRCfe30ioMDPDedRIfNevx7yO4fzxO3Zy/YXepJw8eV/nl6QHibGBYzWwCQhFv3qqsIdUhiISUtlwNITToQXnlNQ6GwJHVkzVCxwAWo0ZH/duypQuhlIvp0Lj6LvoECHRyfd9fuuHmuK/dSsOffqobRmhodwYMpSoxYtRsrLu+xqSVN1pjTlIUZTAMu6HVIylB6/x4a7/ABjatjbNvB3zHaPJFThSTp/B7vGquSdTCMHUrvVxtbfkve1n0SlwIzqZPov+ZkVgGx7ydriv85vZ2uL58UfYdmjP7dlz0CUkQFYWkV9+RdLfh/Cc9ynmuYpHSZKUV4lXVUkVo4mn4ZflvvMRFFRHJfcdR9S33xL3865y6VtZGdq2DouGtsy3UfCvIgpclYTDc8/ht20b1g8/rLYlHzvGtRd6E/+rHHmVpMLIwFFFtPatiWN2Vb3b8amcDYvPf9A9wcSmTev8x1QxzzTxYN3o/BsFt/1buoqC97Lw9qLOmtW4vPyymq5EFxdH2OQp3HpvFrrk+x8ek6TqRgaOKkKrMePJBoad5L+dz1/gybpVyzyvb44aTejkKQDc3bCR2x98WGU2B+bWyle/UdAze6Ngpk5h6sZTfH/gaoF3XiUltFpcX5lEnTWr0XoahqhiN23ieu8+cuJcku5RKQKHEOIDIcRpIcTJ7BVantntQgjxtRDiSvb7j1R0XyvSU7mWqv56/k6+9+07d6bm0KFY+PsDkHb5MqkXL5By8iS3Z8/m7rp1RH33Xbn115T0GwU75KkoOHfPBd7/+fx9bRTMzaZlS/x/+gn77t3UtvTgYG4MHqKv81GF9sZIUlmqFIED+ExRlGaKorQAfgbey27vDtTLfowFFlVQ/yqFx+u7YqHR/ye7cDshTz1vAGFujsc7M/HbslltywgNI3r5CvV17KZN5dPZMuDhYMXGce141M9QUXDF3zd4ZcO/pGWaZjWUpkYNvL74gloff4yZra2+UacjetFirg8cSOqlSya5jiRVZZUicCiKknvA3hb9nhGAXsBqRe8I4CiEeGCXu9hZank0VxnWgup0AJjZ2KB1yx7Wysoi7eLF8uheuXCwNmfVyLwVBXedvsWI5UeJTzVNDiohBI59euO3fXueErVp5//jRt9+RC9bLpftSg+0ShE4AIQQHwkhbgJDMNxxeAG5kwqFZrc9sJ7MlTF3fyGBA8CijqHaXnpwcJn2qbzlVBQMbO+rth25FsOLiw9zOy7VZNex8Pai9qqVuL31JsLCAgAlI4OIzz4jZEQg6aGmmaCXpKqm3AKHEGKfEOJsAY9eAIqizFQUxQdYB0wqxfnHCiGChBBBkZGmWa5ZGT15T2XAPWdusebwDcLvGbay8K1DQZzHjTPJhHJF05gJZvVozJvdGqptF24n0HfRIa5EJJjsOsLMDOfAQPy2/ohV48Zqe3JQENd79iJm3ToU3f3l05KkqqbcAoeiKE8pitK0gMf2ew5dB/TNfh6Gvu5HDm8K2aGuKMoSRVFaKYrSyjVXIZ/qpo6zLXVdbdXXE9ad4N3t5/LtJs99x5Gb85gxCCHKtI/lRQjBhM51+eLF5mizSwqGxabQd9FhjgebNleXZUAAvhs34DJxopqqXZeczJ0PPiR4+HDSrl836fUkqTKrFENVQoh6uV72Ai5kP98BDM9eXdUWiFMU5YEv5davpQ+D2viodSwAQu/ec8eRq15HDo2LCxo723ztVV2fR7xZFtgaGwv9L/S4lAwG//APv57Lv2T5fghzc1wnv4Lv+v+pK9cAUoKOc/2F3kQvXSrL1EoPhEoROIBPsoetTgNPA1Oy23cD14ArwA/oi0Y98CZ0rsvcPs14qYOv2nZv4LBp3QbMzfO0WdSuTXXVqb4rG8a2xcVOPxeRlqlj/NrjrPvH9PM71s2a4bdtK87jx6l3H0paGhGfz+fGgIGkVqPFCJJUkEoROBRF6Zs9bNVMUZQe2RUGyV5N9bKiKHUVRXlIUZSgiu5rZeJd00Z9Hno37w5njZ0tNo8Ytr04vPACblNfLbe+VYRm3o78OKE9dZz13xedAjO3neWLXy+afF7HzNISt1dfxW/LZiwbN1LbU8+d43rffkR+/XWVqokiSSVRKQKHVDreNa3V5/fecQCGJIcaDUn//EPEgi+5+kw3YtatK68ulrs6zrb8OKE9zXIlQvz6jyu89eMZMu+zKFRBrBo1wm/jRlynTVNXXpGZSdTCRVx/oTdJR46Y/JqSVNFk4KjC8t5xpOT7q7pG9254ffUV9Y8cpubAgaScOEF6cDAZ4eHl3dVy5WJnyfoxbelU37BIYmPQTcauOU5yuunnIIS5OS5jx+D30zasc93lpV+7RkjgS4S9/gaZ1Xiln/TgkYGjCqtpY65OCCemZRKXkncDnLmnJzWeeRqNvT1aZ8PGwazoqlUdsDRsLbUsHdGKvo94q21/XIhg8A//EJNUNkNIlv7+1Fm7Bvd33jHsOgfid+7kavdniVm7Tm4clKoFGTiqMCFEscNVOTS5qwNW0SJPJWWuMePz/s14+QnDCrOTN2Ppt+gQN2PKJuutMDPDaegQ/Hfvpsazz6rtusRE7nz4Idf795dJE6UqTwaOKs6niAny3B60O44cQghef6Yh7/dqQs72lWtRSfRZdIizYXFldl1zdze8vphP7eXLsPD1VdvTzv/HjYGDCHvt9Wo/ZChVXzJwVHFG33E4577jeHACR47h7XxZOPgRLLKLQkUmpDFwyRH+73JUmV7Xtn17/HZsx/XVKQhLS7U9/uefudr9WSK++gpdUlKZ9kGSTE0Gjiquvoc9LXwc6dXCE3/Xwjf3aZ1y33FEV4u0IyXV/aFarBnZBnsrfcXkxLRMXlp5lO0nC0xGYDJmFha4jB+P/66fsX/6abVdSUsjetFirnbrTuyPW2XqEqnKENXxF0irVq2UoCC55eNeFx5+BCVFf1dSP+gYGju7Cu5Rxbh4O4ERy49yO96QEHHms40Y87h/EZ8yneRjx7gz9xNSz5/P027ZoAGuU6Zg90TnapMWRqpahBDHFUVpVdxx8o7jAXLvXceDqoGHPVsntqeemyFwfrT7Pz4wYVGooti0bo3vls3UmjvXkP4eSLt4kdCJE7kxcCBJhw49kHeFUtUgA8cDJM88xwM0QV4QT0drtoxvTxtfQzBd9n/XmbLxpMmKQhVFmJnh2PsF6v6yB5eJExHWhrmq1FOnCRk5ipDhI0g+caLM+yJJJSUDxwNE42DYTZ0VF1vEkQ8GBxtzVo9qQ7cmhqJQO0+FE7j8mMmKQhXHzMYG18mvEPDbr9QcPgyRK79Y8rFjBA8eQsjoMSQdPSrvQKRKQ85xVAP7L0RwPPguwTHJjOnoRzNvxwKPSz5xAl1iIpoaNbCoWxeNvX2Bxz1osnQKs3ecY80RQ0LEhh72rBrZBvcaVkV80vQybt0iatFiYrduhXsy7Vo3b47zmNHYPfkkwkz+zSeZnrFzHDJwVANTN55k27/6lUFz+zzEoDbVNwtuWVEUhYV/XuWzvYbMtl6O1qwe1Ya6ruW/iCA9JISo774jbsdOuOf/UQt/f5xHj8bh+ecM+bEkyQTk5PgDpLaTYRNgcHTZ7Iiu7oQQvPxEAJ/1a4YmT1GoQxwPvlvu/bGoXfaBR9UAABwYSURBVBvPTz/Ff/cuHPv3zzOElX7tGrdmzODKU12J/O47mQdLKncycFQDOWnEAUJi5Gay+9G/lQ9LR7TC2lyfAyw2OYMhS4+w7/ydCumPpZ8ftT54n7q/78N59Kg8ObAyIyKI+uZbLj/ZhbDpr5F84oScB5HKhQwc1UDuwGHMHYeiKCiyVkShnmjgxoaxbXG21Q8DpWboGLsmiA1HQyqsT+Zubri99hoB+//Addo0NK4uhjczMojftYvgwUO43rMn0StWkvkAL7eWyp4MHNVAbSfDX6Eh0cmF/tWZdPQolzp25GKz5tycNKm8ulclNffRF4XKGQbUKfDW1jN8ue9Shf5Vr6lRA5exY6j3++94zv88Txp3gLTLV4j49FMud+rMzUmTSPj9d1lQSjI5GTiqARc7CzW9ekJaZqE5q4TWnKzIKJSMDLLiyi7BX3Xh66IvCtXUq4ba9uW+y8zYVjZFoUpCWFjg8Nxz+P5vHX7btuLQry/CxnDnSWYmift+J/TlSVxu34HwN98i4c8/5Z2mZBIycFQDQgha5drIlrPC6l4aR8M+Dl2sDBzGcLW3ZMPYdnSsZxgaWn9UXxQqMc30RaFKw6pRIzw//JB6f/1FrY8+zHcXoktMJG77dkLHT+BSh8cIf+ttEvbvR5dSeFJMSSqKDBzVRN9HvNTnW46HFjickncDoAwcxrKz1LJsRGt6P2z4Hv9xIYJ+iw4RHlt5fvlq7Gxx7NsX3/+tw3/3bpzHjsXcxyfPMbqEBOJ++onQCRO51LYdIWPGErNmLenBwYWcVZLyk/s4qonUjCxaf7SPhFT9X8Ebx7blUX/nPMcoGRlceKiZ/oWZGQ3PnpEbyUpAp1P47NeLLPrzqtrmam/J0uGtaO5T8KbLiqYoCqnnzpOw9xfi9/xCRmhoocea16mN3WMdsWnTBptWLdE6Oxd6rFQ9yQ2AD1jgAJi57Qzr/tGv/HmjWwMmdg7Id8zFR1qiS9avvKp/7KjcPV4Km4JuMmPrGTKzEyJamZux4MUWdH+oVgX3rGiKopB69hzxv+wh8cAB0q9cLfJ4C39/bFq2xKZ1K2xatkTr6Smz9lZzMnA8gIHjfHg8By5F0rOFJ16O1gUec/nJJ8kMvwVA3X2/YeHtXeBxUtEOX41m/Nrjeeq8v/5MAyZ2rltlfrlmhIWRePAgiX8dJOnwYTXlfmE0Tk5YNWmCVZPGWDdtilWTJmg9PKrM1ysVTwaOBzBwGONa7z6k/fcfAL5btmDdtEkF96jquhaZyKhVQVyPMmy67NfSm497P6RWGqwqdOnppAQFkfTPUZKDgkg9fRolo/hEjxonJyzr1cMyIADLgLpYBgRgERCAtmbNcui1ZGrGBg5teXRGqjxkhlzT8Xe1Y9vE9oxbc5x/ruvT1G85HsrNmGQWDW2Jk23VySNlZmGBbfv22LZvD4AuLY3U06dJDgoi+dgxUk6fQZeYmO9zWTExJP/zD8n//JOnXePkhEXt2pj7+GDh4425d86/3mjd3eXcWhUn7zgeMKFTXiVh714AvL6YT41nn63gHlV96Zk6Zm47w+bjholnL0drlgxvSRNPhyI+WXUoOh3pwcGknjtP6tmzpJ47R+r586Wqly7MzdHWqoW5mxtaNze07u76f91cMc9+rnFyxszWRg6DlTN5x/GAuxKRyPaTYTSqVYNnc03aamoYNrNlxcdXRNeqHQutGfP6NcPf1Y5Pf7kAGBIkzuvXnJ7NPSu4h/dPmJlh6eeHpZ8fDs8/B+iDSUZoKGlXrpJ25QrpV6/on1+7VuR8iZKRQUZICBkhxaRwMTdH6+iIpmZNNDn/1nRE4+iItmZNzOxrYGZni8bODrOch60dGjtbhI0MOmVJ3nFUQ9v+v707D4+qvBc4/v3NTJIhiwRIQpBFwqoIKgiIG3BblKValLohrUtRbx+Xutai+KhXb11uN6tVK4i1WluwV3Gr9VpxAVFAFAKIsgUkgZCwJCH7bO/945xMZgZCMkBmhszv8zzznHPeOTPzyzsn85vznnfed1UJty8oDG6P6JPNuEF53DphIN4dOwg0NuLs3Bln586IS787HE0frC/jtgWrw34c+J/j+nH3xBODo+52dCYQwFdaiqdkB96SYjzFxXiLS/CUWEv/vhjMPikSkkzScaS5Ebcbh9tepqXZ22lImhtxp+FwdwrfTksDlwtJSUFcKdYypWm7eUkL9+F0HnPJSy+OJ3HiKK9uYMwji4icPnvRnePiMrdEstlcXsMNL62kKOSi+bkDc3hq+nCy04+d6x7txV9Ti69sF76yMrzl5fjKd+MrL8dXVoavvBzv7nL8FZWt9vI6Jrhc1vUch8NaOp3B7eB6U4IJ2cYhiMPZ+j4iIA4QAYeACCIOcm66iU7DhkYdrjZVJbG8LDdnD8hhyaY9YeWbyqo1ccTAgLxM3rj5bG6bv5oPvy0HYMmmPUx9einPzjidIccf18ozdGzOzAycmf1J69//kPsFGhrwV1bir6iwbpWV+Coq8FdU4q+sJFBdjb+2hkBNLYGaGgI1NcHthEk6Ph9N399i+RW9y5XT2/X5NXF0UBcP73lA4timkzzFzHHuFJ6/aiS//2AjT324GbCGvL/omaU8eOHJTB/d+5hrxog1h9uNIz+flPz81neOYHw+ArW1djKpxTQ2Eqivt5YNDZiGRkxjA4GGRkxDvbUMbjdY+3g8GJ8P4/VYCcDjtbZ9Poy3ad1rdVv2RpR7veD3t0OttJG0b681TRwd1EWn9eSb0v3MXbI1WPbdXqvpxBiDt7iYhvXfcNykifEKscNzOIQ7zx/Myccfxx2vFlLn8ePxBbh34VqWb93LIxcPIyNN/wXbg7hcwet4Ka3v3i5MIAB+v7UMBDD+AAT8GL8fmspC9wnb1x+yNPbjAmFLjMEEjDW1sAnY2wEw4B5yUrv+bXrUdlAOhzD7B0M4q38O1774BQDb9tRhfD62TJocHLMo/fPP9Mda7WzS0B4M7J7FTa98xbe7qgF4c/VO1u6o4pkZIzgxP7mbrjqq4LWNeAfSDvRXOB3cgLxMJpzUnevOKeDyUb0Rlyts8Lq6JO5EEEv9czNZeOPZXD6yebTaot21TP3jUhZ8sV2nfFXHlIRKHCJyp4gYEcmxt0VEnhSRzSKyRkRGtPYcKlzvruk8f/VI7rtgCBfZw4Knjx4dvL/uiy/iFVrS6ZTq5PFLTuF3l50anNO80Rfgl6+t5Za/r6KqrvUhPpRKBAmTOESkN3A+EPqroMnAQPt2A/BsHELrcNJHjwqu163QxBFr00b04u1bzmZQ9+Yebu+sKWXiE4tZunnPIR6pVGJImMQB/B64m/Bea1OBl4xlGZAtIok9dvUxoNPwEVY/cKBxwwZ8FRVxjij5DMjL4o2bzuaKUc1NV7v2NzDj+eU89PZ6Grxx7JGjVCsSInGIyFRghzGmMOKunkBxyHaJXaYOQ6PPj9cfwJmZgbtpVFxj2POMnsjFQ3qqi8d+dArP/SR8QMQXlm7lwqc+5avtmtBVYopZryoR+QA4WIfs2cC9WM1UR/L8N2A1Z9GnT58jeaoOZ3VxJfNXbOfVlcUEDEw6OZ9Hp1/JrsI1AFS8/DIOdxpZEybQ6dRT4xxt8pl4cj7D+2Qz67W1wR8Mbiqv4UfPfsY1Z/XlrvMHa7ddlVBidsZhjJlgjBkaeQOKgAKgUES2Ab2Ar0QkH9gBhE6a3MsuO9jzzzHGjDTGjMzNzW3fP+YYU1HrYf4XxcEhSN77ehfrh55FxrixwX32zn2e0gf/K04RqrwsN/OuHsmvLh5KeqrVjGgM/HnpNiY+sZglm3bHOUKlmsW9qcoYs9YYk2eM6WuM6YvVHDXCGLMLeAu4yu5dNQaoMsaUxjPeY9HYQblMGRZ+sldYUkWPhx7CFfKrXP0lc3yJCDPOOIH3bx/L2EHNX35KKur5ybwV3Dp/FbuqGuIYoVKWuCeOVryLdUayGZgL3BjfcI5NTofwzIzTeWzasGDZmuIqUrp3p98bC8l/4H6yzpuAe9iwQzyLipVeXdL5y7Wj+N1lp5Kd3vy75zdX7+R7v/2YZz/eQqNPL56r+NHRcZPI+p37mfLkEgCO7+zms3u+H+eIVGt2Vzfy0DvrebtwZ1h5QU4G918whPGDc/VMUR01bR0dN9HPONRRNKh7Ju4U6y3fWdXA7urGOEekWpOblcZT04fzt+vPCPvdx9Y9tVz74hdMn7tMe1+pmNPEkURcTkfYVKZrSsLnHG/csoW98+axc/ZsKha8Guvw1CGc1T+Hf/78XO6/YAhZ7uYeVsuK9jHtmc+44aWVbCyrjmOEKplo4kgyp/RqThyFJVVh9zV8/TXlv/4NVa+9Tu2nn8Y6NNWKFKeDn55TwEd3jefKM/qEzSj4/voyJj2xmJ//fRXrd+qUwKp9aeJIMqGJY+W28Ck8Uwv6Bdcbi4piFpOKTk5mGo9cPIwP7hjHhSHzmQcMvFW4kylPLuGaP69gedFeHTxRtQtNHElmTL9uuBzC2EG5XD6qd9h9qQUFwXXP9u3WZDQqYRXkZPDU9OG8c8s5jB8c/tuljzfs5vI5y5j27GcsXFWiQ5ioo0p7VSWhqjovndMPPr3NpnHj8ZWVAdDvX++SFpJMVGJbW1LFnz7ZwrvrSon8t+6SnsKlI3tz5eg+9M3JiE+AKuFpryrVopaSBkBa/5Dmqg0bYhGOOkqG9erM0zNG8OGd45k+ujepzuZ/74o6L3MWFzH+Nx9zxZzPmb9iuw7jrg6bJg4Vxj3slOB61TvvxDESdbgKcjJ4dNopLJ31PX4xcTA9szuF3b+saB+zXl/LqF99wA0vreSdNTupbtAkotpOm6qSXIPXz1ffVXDWgBwAGou2UjRlinWny8XAjz/ClZMTxwjVkfIHDJ9sLOevy7bz0YbyA5qxAFKcwph+3fjeiXlMOKk7vbumxz5QFXdtbarSxJGkAgHDnCVFzPt0KxW1Hj65+z+C30y3zfgx9V9+CUDeL+6i28yZ8QxVHUXl+xt4e00pb67ewZqI7tih+uVkMKZ/N8b068aZ/bqRm5UWwyhVvGji0MTRqsuf+5zlW60uudNH9+bRaVYzVeXrCym9914AnDk59HtjoZ51dEBFu2t4q3An739dxvrSQ//2o39uBsP7dOHU3tmc1iubwflZpLq0pbuj0cShiaNVizfu5qoXVgS3n79qJBOGdCdQV8fmCefh37ePzPHjOf6xR3FmZ8cxUtXedlbW8+G35Sz6poylW/bi8QUOuX+qy8GJ+VkMzMticH4mA7tnMbh7Fj06u3XsrGOYJg5NHK0yxnDDy1/y7/VW99su6Sm8fcs59OqSTs2ST2lYv55u11+HOPSbZTJp8PpZtb2Sz4v2sqxoL6u3V+LxHzqRNMlKc9EvN4NeXdPp3SWd3l070cde75HtJs3lbOfo1ZHQxKGJo00qaj1M/sMSdu235nk4MT+Lf/zsTLLcB++y6/nuOzCG1L59Yxiliqd6j591O6soLK5kdXElhSWVFO+rP6znyk5PIS8rjbwsN3lZaeQeZ63nZqXRJT2F7E6pdO6UQudOKWS5XTgcevYSS5o4NHG02cpt+5g+dxlev3UsTDgpjzk/GXnAP62/poZtl12Ob88eevz3w2Sdd542SySpfbUeNuyqZmOZddtUVsOGsmqq6o9et14R6wymc7qVSLI7pZKR5iQ91UWnVCfpKU7SU510SnXZSycZ9ro7xUmqy0Gay0GK00Gqy0GKU0h1OUgNbjtwOUSP4RCaODRxROV/vyzhrn8UBrdvnzCIWycMDNun5LbbqX7vveB2+hlnkHfH7TpPuQKsps/d1Y18t6+O4n11bN9XR/G+eor31VFcUUfZ/obg9MWJQsQaPDLN6SDFZSUSl0NwOASnQ3CKvXQIjpD1pvscDuxtB04huJ/LaS0dIoiAAA57RQgvE7HiwC53ROwjwecIeZxDsB8SLA993MXDe9IvN7PlP7zF+mhb4nC1toNKDpec3ouNZdXMWWwNbvjEoo30zUln6mk9g/t0mzmT+lWrgkOS1C1fzvafzmTAJ5/gzNRhLJKdiJB3nJu849yM6tv1gPv9AcPe2kbK9zeyu7qR8uoGyvc3Ul5tbVfVe6ms97K/3ktVvZeaRl+7x2wMeHwBqzNAB5qe5vQTuhxW4mgrTRwq6O6Jg1m3o4rPtuzFGNgf0ezQadhQCt5YyJ6n/kjFggXg99P1mms0aag2cTrEvrbhbtP+Pn+A/Q0+quxEUlXvpa7RR63HT73HR53HT53HT73XT529Xe/xU+vx0+Dx4/FbCcHrD+DxB/D6AsEyjz+A12/wJ9op0FHS3s1v2lSlwlTUerjy+eXMPKeAS07v1eJ+jVu3snfePLrPmoUzs/2+2SjVnvwBg9cfoNFOMF5/AH/AEAiA31iJJWAMPr+19AdMsNzaz9r2Na037W+vGwMGe2kgYAwGIKQ8ELoPgDFWmb1v0+Psuw76OLB+1Nu0/0XDj+eEbtF/odOmKnVYumSk8vbNZ+NyhnfBbfT5uf6lL+mZ7QaERp+fmkEXMW1bNZOGauJQxybr+oR1MV21nSYOdYDIpAHwbWk1izfuPqD85OM7M2loLKJSSiUK/WWXapPCiPnJmxzN7pdKqWODnnGoNpk0NJ+czDQq6jwEDLhdDjLSXAzM02YqpZKNJg7VJnlZbqYM6xHvMJRSCUCbqpRSSkVFE4dSSqmoaOJQSikVFU0cSimloqKJQymlVFQ0cSillIqKJg6llFJR6ZCDHIrIbuC7w3x4DrDnKIZztCRqXJC4sWlc0dG4otMR4zrBGJPb2k4dMnEcCRFZ2ZbRIWMtUeOCxI1N44qOxhWdZI5Lm6qUUkpFRROHUkqpqGjiONCceAfQgkSNCxI3No0rOhpXdJI2Lr3GoZRSKip6xqGUUioqmjhCiMgkEdkgIptFZFYc4+gtIh+JyHoR+VpEbrXLHxSRHSKy2r5NiUNs20Rkrf36K+2yriLybxHZZC+7xDimwSF1slpE9ovIbfGoLxF5QUTKRWRdSNlB60csT9rH2xoRGRHjuH4tIt/ar71QRLLt8r4iUh9Sb3+KcVwtvm8ico9dXxtEZGKM41oQEtM2EVltl8eyvlr6bIjtMWaM0ZvVXOcEtgD9gFSgEBgSp1h6ACPs9SxgIzAEeBC4K871tA3IiSj7H2CWvT4LeDzO7+Mu4IR41BcwFhgBrGutfoApwL8AAcYAy2Mc1/mAy15/PCSuvqH7xaG+Dvq+2f8DhUAaUGD/vzpjFVfE/b8F7o9DfbX02RDTY0zPOJqNBjYbY4qMMR5gPjA1HoEYY0qNMV/Z69XAN0DPeMTSRlOBv9jrfwEuimMs3we2GGMO9wegR8QYsxjYF1HcUv1MBV4ylmVAtoi0y2xZB4vLGPO+McZnby4DerXHa0cb1yFMBeYbYxqNMVuBzVj/tzGNS0QEuAz4e3u89qEc4rMhpseYJo5mPYHikO0SEuDDWkT6AsOB5XbRzfYp5wuxbhKyGeB9EflSRG6wy7obY0rt9V1A9zjE1eQKwv+h411f0HL9JNIx91Osb6ZNCkRklYh8IiLnxiGeg71viVJf5wJlxphNIWUxr6+Iz4aYHmOaOBKYiGQCrwG3GWP2A88C/YHTgFKs0+VYO8cYMwKYDNwkImND7zTW+XFcuuqJSCrwQ+AfdlEi1FeYeNZPS0RkNuADXrGLSoE+xpjhwB3A30TkuBiGlHDvW4TphH85iXl9HeSzISgWx5gmjmY7gN4h273ssrgQkRSsA+MVY8zrAMaYMmOM3xgTAObSTqfph2KM2WEvy4GFdgxlTae/9rI81nHZJgNfGWPK7BjjXl+2luon7seciFwDXADMsD9wsJuC9trrX2JdSxgUq5gO8b4lQn25gGnAgqayWNfXwT4biPExpomj2RfAQBEpsL+5XgG8FY9A7DbUecA3xpjfhZSHtk1eDKyLfGw7x5UhIllN61gXV9dh1dPV9m5XA2/GMq4QYd8E411fIVqqn7eAq+yeL2OAqpDmhnYnIpOAu4EfGmPqQspzRcRpr/cDBgJFMYyrpfftLeAKEUkTkQI7rhWxiss2AfjWGFPSVBDL+mrps4FYH2Ox6AlwrNyweiBsxPrGMDuOcZyDdaq5Blht36YALwNr7fK3gB4xjqsfVq+WQuDrpjoCugGLgE3AB0DXONRZBrAX6BxSFvP6wkpcpYAXqz15Zkv1g9XT5Wn7eFsLjIxxXJux2r+bjrE/2fv+yH5/VwNfARfGOK4W3zdgtl1fG4DJsYzLLn8R+FnEvrGsr5Y+G2J6jOkvx5VSSkVFm6qUUkpFRROHUkqpqGjiUEopFRVNHEoppaKiiUMppVRUNHGopCIiL4rIB/GOI5KIfCwiz8c7DqXaQrvjqqQiIp0BhzGmwv6gHmCMGR/D178PuM4Y0zeivCvgMxHDRyiViFzxDkCpWDLGVLXH84pIqrFGVT4sxpi2jhCrVNxpU5VKKk1NVSLyINavlMeJiLFv19j7ZIrIH8SaTKjOHvV0Wshz9LX3nyEi74pILfCwPazDXBHZItbEPkUi8oiIpNmPuwZ4GDgh5DUftO8La6oSkRQRecyOwSPWxD1XRvwtRkRuFJGXRaRaREpE5J6Ifaba8deJSKWIrBCR4e1QtSqJ6BmHSla/wRpTqABr0DqAKnssoLexhmq4HNiJNT7RfBGZbIxZFPIcjwO/BG6ytwVrcLkrgTLgFOA5rGErHsAaGO9EYAYwyn5MTQvxPYI11PnPsIZ4uQT4q4iURcTwAHAf1uRHk4A/isgKY8wiEcnHGin4PnvpxhqG24dSR0ATh0pKxpgaEakHPMaYXU3lIjIeOBNrfoOmZq059gBxt2CNB9TkOWPMK4SbHbK+TUT6AzcCDxhj6kWkBvCHvmYkEUkHfg7cboxpGiL+EREZZT9/aAwLjDFz7fWnReRmrES3CGu2uBTgVWPMNnufb1p6XaXaShOHUuFGYU0dvMM6+QhKxRpALtQBI7OKyPXAdVjTiWZg/Y9F2yQ8wH69xRHlnwD3RJStjtjeSfMkPmuA/wPWici/gY+B140xxSh1BDRxKBXOAVTR3JQUKvLid23ohohcijUS6SysD/n9wKXAr45+mC3GZLATlTHGLyKTsf6WCVijuD4mIpcaY95px5hUB6eJQyUzD+CMKFsJZANuY0y083eMBVaZ8DlU+rbhNSNtBhrt5wuNYRxRzilirP72K+zbIyLyHnAtoIlDHTZNHCqZbQUuFZGTsS5mVwMfYs1n8LqI3I3V3NMFOAtoCLmecDAbgJkiMhXrA/4Cmi+8h75mvoicidX0VWdCJlECMMbUiciTWD21dtN8cXwqcF5b/zgROQv4PvA+1twSA7Eu2M9r63ModTDaHVcls3lYMz9+BuwGptvf0H8IvA78HvgW+CfwA6zJcA7lOaxJiP4MrALOwOrtFOoNrB5O/7Rf8+4Wnms21rSpT2AloR8DP47oUdWaKqwL/W9iJakXsOYVfziK51DqAPrLcaWUUlHRMw6llFJR0cShlFIqKpo4lFJKRUUTh1JKqaho4lBKKRUVTRxKKaWioolDKaVUVDRxKKWUioomDqWUUlH5fyLWYszADMK0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.rc('text',usetex=True)nn\n",
    "#plt.xscale('log')\n",
    "long_end = 200\n",
    "x_long = [i for i in range(long_end+1)]\n",
    "plt.plot(x_long,origin_DGD_error[:long_end+1],linewidth=3,color = 'tab:red')\n",
    "plt.plot(x_long,origin_PGEXTRA_error[:long_end+1],linewidth=3,color = 'tab:blue' )\n",
    "#plt.plot(x_long,origin_NIDS_error[:long_end+1],linewidth=3)\n",
    "\n",
    "x = [i for i in range(num_layers+1)]\n",
    "plt.plot(x,pred_DGD_error[:num_layers+1],linewidth=3,linestyle='--',color = 'tab:red')\n",
    "plt.plot(x,pred_PGEXTRA_error[:num_layers+1],linewidth=3,linestyle='--',color = 'tab:blue')\n",
    "#plt.plot(x,pred_NIDS_error[:num_layers+1],linewidth=3)\n",
    "\n",
    "plt.legend(['Prox-DGD','PG-EXTRA','GNN-Prox-DGD','GNN-PG-EXTRA'],loc='upper right',fontsize='large') \n",
    "plt.xlabel('iterations',fontsize= 'x-large')\n",
    "plt.ylabel('NMSE',fontsize= 'x-large')\n",
    "\n",
    "figure_name = \"D\"+str(n)+\"M\"+str(m)+\"NO\"+str(nnz)\n",
    "plt.savefig(\"./error_fig/noise3/\"+figure_name+\".eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
